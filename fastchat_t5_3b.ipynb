{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWNry4vhWuJN",
        "outputId": "89384c37-b600-402f-a4dc-3730e8860ca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting huggingface_hub==0.16.4\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (3.17.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (4.67.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (4.12.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.16.4) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.16.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.16.4) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.16.4) (2025.1.31)\n",
            "Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.28.1\n",
            "    Uninstalling huggingface-hub-0.28.1:\n",
            "      Successfully uninstalled huggingface-hub-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "diffusers 0.32.2 requires huggingface-hub>=0.23.2, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "transformers 4.48.3 requires huggingface-hub<1.0,>=0.24.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "accelerate 1.3.0 requires huggingface-hub>=0.21.0, but you have huggingface-hub 0.16.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface_hub-0.16.4\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub==0.16.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsOtVCSMW5rv",
        "outputId": "1f88370f-e58d-4996-9f3d-26105e353112"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: langchain 0.3.20\n",
            "Uninstalling langchain-0.3.20:\n",
            "  Successfully uninstalled langchain-0.3.20\n",
            "\u001b[33mWARNING: Skipping langchain-community as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: langchain-core 0.3.44\n",
            "Uninstalling langchain-core-0.3.44:\n",
            "  Successfully uninstalled langchain-core-0.3.44\n",
            "\u001b[33mWARNING: Skipping faiss as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping faiss-cpu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping faiss-gpu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping InstructorEmbedding as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y langchain langchain-community langchain-core faiss faiss-cpu faiss-gpu InstructorEmbedding numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJFw1jY9XC2r",
        "outputId": "b6465912-0ec6-4bc7-f804-00c61959a43e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain==0.1.10\n",
            "  Downloading langchain-0.1.10-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain-community==0.0.27\n",
            "  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.10) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.10) (2.0.39)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.10) (3.11.13)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.10)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.10) (1.33)\n",
            "Collecting langchain-core<0.2,>=0.1.28 (from langchain==0.1.10)\n",
            "  Downloading langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.10)\n",
            "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.0 (from langchain==0.1.10)\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy<2,>=1 (from langchain==0.1.10)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.10) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.10) (2.32.3)\n",
            "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.1.10)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.10) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.10) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.10) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.10) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.10) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.10) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.10) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.10)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.10)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.10) (3.0.0)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.28->langchain==0.1.10)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain==0.1.10) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain==0.1.10) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain==0.1.10) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.1.10) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.1.10) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.1.10) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.10) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.10) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.10) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.10) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.10) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain==0.1.10) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain==0.1.10) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain==0.1.10) (0.14.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.10)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain==0.1.10) (1.3.1)\n",
            "Downloading langchain-0.1.10-py3-none-any.whl (806 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.2/806.2 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.0.27-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.1.53-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.1/303.1 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
            "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: tenacity, packaging, numpy, mypy-extensions, typing-inspect, marshmallow, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.13\n",
            "    Uninstalling langsmith-0.3.13:\n",
            "      Successfully uninstalled langsmith-0.3.13\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.6\n",
            "    Uninstalling langchain-text-splitters-0.3.6:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "diffusers 0.32.2 requires huggingface-hub>=0.23.2, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "transformers 4.48.3 requires huggingface-hub<1.0,>=0.24.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "accelerate 1.3.0 requires huggingface-hub>=0.21.0, but you have huggingface-hub 0.16.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-0.1.10 langchain-community-0.0.27 langchain-core-0.1.53 langchain-text-splitters-0.0.2 langsmith-0.1.147 marshmallow-3.26.1 mypy-extensions-1.0.0 numpy-1.26.4 packaging-23.2 tenacity-8.5.0 typing-inspect-0.9.0\n",
            "Collecting accelerate==0.25.0\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting transformers==4.35.2\n",
            "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.41.2\n",
            "  Downloading bitsandbytes-0.41.2-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (0.16.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (3.17.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.35.2)\n",
            "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->accelerate==0.25.0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->accelerate==0.25.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.25.0) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.35.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.35.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.35.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.35.2) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.25.0) (3.0.2)\n",
            "Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.41.2-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, accelerate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.0\n",
            "    Uninstalling tokenizers-0.21.0:\n",
            "      Successfully uninstalled tokenizers-0.21.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.3\n",
            "    Uninstalling transformers-4.48.3:\n",
            "      Successfully uninstalled transformers-4.48.3\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.3.0\n",
            "    Uninstalling accelerate-1.3.0:\n",
            "      Successfully uninstalled accelerate-1.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.25.0 bitsandbytes-0.41.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tokenizers-0.15.2 transformers-4.35.2\n",
            "Collecting sentence-transformers==2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting InstructorEmbedding==1.0.1\n",
            "  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.14.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (3.9.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.17.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.32.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.12.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.5.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->sentence-transformers==2.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->sentence-transformers==2.2.2) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2025.1.31)\n",
            "Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=b2cdff5411e507e5f1d760b8028828792e84deb4fdc1b32c5d6fffccaf11c921\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/27/bf/ffba8b318b02d7f691a57084ee154e26ed24d012b0c7805881\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: InstructorEmbedding, sentence-transformers\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 3.4.1\n",
            "    Uninstalling sentence-transformers-3.4.1:\n",
            "      Successfully uninstalled sentence-transformers-3.4.1\n",
            "Successfully installed InstructorEmbedding-1.0.1 sentence-transformers-2.2.2\n",
            "Collecting faiss-cpu==1.7.4\n",
            "  Downloading faiss_cpu-1.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Downloading faiss_cpu-1.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu==1.7.2 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu==1.7.2\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting pymupdf==1.23.8\n",
            "  Downloading PyMuPDF-1.23.8-cp311-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.23.7 (from pymupdf==1.23.8)\n",
            "  Downloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Downloading PyMuPDF-1.23.8-cp311-none-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.23.7 pymupdf-1.23.8\n"
          ]
        }
      ],
      "source": [
        "# LangChain\n",
        "!pip install langchain==0.1.10 langchain-community==0.0.27\n",
        "\n",
        "# LLM and Transformers\n",
        "!pip install accelerate==0.25.0 transformers==4.35.2 bitsandbytes==0.41.2\n",
        "\n",
        "# Text Embedding\n",
        "!pip install sentence-transformers==2.2.2 InstructorEmbedding==1.0.1\n",
        "\n",
        "# FAISS Vector Store (Install CPU version first, then upgrade to GPU)\n",
        "!pip install faiss-cpu==1.7.4\n",
        "!pip install faiss-gpu==1.7.2 --no-cache-dir\n",
        "\n",
        "# PDF Processing (if needed)\n",
        "!pip install pymupdf==1.23.8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93iuj1-3YKUx",
        "outputId": "cf4a1668-bba4-494d-f15c-9b9c460f3ee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Downloading pypdf-5.3.1-py3-none-any.whl (302 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/302.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2VJlATJYKSG",
        "outputId": "09ef83fb-9bbb-4fb0-c1b6-ac12714494e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: bitsandbytes 0.41.2\n",
            "Uninstalling bitsandbytes-0.41.2:\n",
            "  Successfully uninstalled bitsandbytes-0.41.2\n",
            "Collecting bitsandbytes==0.41.2\n",
            "  Downloading bitsandbytes-0.41.2-py3-none-any.whl.metadata (9.8 kB)\n",
            "Downloading bitsandbytes-0.41.2-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m218.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.41.2\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y bitsandbytes\n",
        "!pip install bitsandbytes==0.41.2 --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9MSK_jkae4fj",
        "outputId": "79b593b6-72b5-45ac-f64e-81115c354345"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: triton 3.2.0\n",
            "Uninstalling triton-3.2.0:\n",
            "  Successfully uninstalled triton-3.2.0\n",
            "Collecting triton==2.0.0\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (3.31.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (3.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (2.6.0+cu124)\n",
            "Collecting lit (from triton==2.0.0)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (12.4.127)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch (from triton==2.0.0)\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Downloading torch-2.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (1.13.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting torch (from triton==2.0.0)\n",
            "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Downloading torch-2.3.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting torch (from triton==2.0.0)\n",
            "  Downloading torch-2.3.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "INFO: pip is still looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting torch (from triton==2.0.0)\n",
            "  Downloading torch-2.2.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Downloading torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "  Downloading torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting torch (from triton==2.0.0)\n",
            "  Downloading torch-2.1.1-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->triton==2.0.0) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->triton==2.0.0) (0.45.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->triton==2.0.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->triton==2.0.0) (1.3.0)\n",
            "Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m314.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m284.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m321.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m321.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m293.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m248.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m363.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m256.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m293.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m291.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m299.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m262.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m322.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m290.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, triton\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\n",
            "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "713e805871444d49a0667de2bbf7b5f0",
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip uninstall -y triton\n",
        "!pip install triton==2.0.0 --no-cache-dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4d6bPNDeuTg",
        "outputId": "e3a63c08-bd3c-4b77-f377-9ed382dcd688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "++++++++++++++++++ BUG REPORT INFORMATION ++++++++++++++++++\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "++++++++++++++++++ /usr/local CUDA PATHS +++++++++++++++++++\n",
            "/usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so\n",
            "/usr/local/lib/python3.11/dist-packages/torch/lib/libc10_cuda.so\n",
            "/usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda_linalg.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda120.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda114.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda121_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda122_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda111_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda122.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda120_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda114_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda115.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda111.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda110_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda110.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda115_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/libucx/lib/ucx/libucm_cuda.so\n",
            "/usr/local/lib/python3.11/dist-packages/libucx/lib/ucx/libucx_perftest_cuda.so\n",
            "/usr/local/lib/python3.11/dist-packages/libucx/lib/ucx/libuct_cuda.so\n",
            "/usr/local/lib/python3.11/dist-packages/cuda/cudart.cpython-311-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.11/dist-packages/cuda/ccudart.cpython-311-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.11/dist-packages/cuda/ccuda.cpython-311-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.11/dist-packages/cuda/cuda.cpython-311-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.11/dist-packages/rmm/pylibrmm/cuda_stream.cpython-311-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.11/dist-packages/pylibraft/common/cuda.cpython-311-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.11/dist-packages/jax_cuda12_plugin/cuda_plugin_extension.so\n",
            "/usr/local/lib/python3.11/dist-packages/jax_plugins/xla_cuda12/xla_cuda_plugin.so\n",
            "/usr/local/lib/python3.11/dist-packages/cuml/common/cuda.cpython-311-x86_64-linux-gnu.so\n",
            "/usr/local/cuda-12.5/targets/x86_64-linux/lib/libcudart.so\n",
            "/usr/local/cuda-12.5/targets/x86_64-linux/lib/stubs/libcuda.so\n",
            "/usr/local/cuda-12.5/compat/libcuda.so\n",
            "\n",
            "+++++++++++++++ WORKING DIRECTORY CUDA PATHS +++++++++++++++\n",
            "\n",
            "\n",
            "++++++++++++++++++ LD_LIBRARY CUDA PATHS +++++++++++++++++++\n",
            "+++++++++++++++ /usr/lib64-nvidia CUDA PATHS +++++++++++++++\n",
            "/usr/lib64-nvidia/libcuda.so\n",
            "\n",
            "++++++++++++++++++++++++++ OTHER +++++++++++++++++++++++++++\n",
            "COMPILED_WITH_CUDA = True\n",
            "COMPUTE_CAPABILITIES_PER_GPU = ['7.5']\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "++++++++++++++++++++++ DEBUG INFO END ++++++++++++++++++++++\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Running a quick check that:\n",
            "    + library is importable\n",
            "    + CUDA function is callable\n",
            "\n",
            "\n",
            "WARNING: Please be sure to sanitize sensible info from any such env vars!\n",
            "\n",
            "SUCCESS!\n",
            "Installation was successful!\n"
          ]
        }
      ],
      "source": [
        "!python -m bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qNLAhPU1euQH"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0f3K_TKkpBK",
        "outputId": "7e693188-35d9-49c8-8abf-c1344518c505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.1.10)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.41 (from langchain)\n",
            "  Downloading langchain_core-0.3.45-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.1.147)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.61.1)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
            "Downloading langchain-0.3.20-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.8-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.45-py3-none-any.whl (415 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.9/415.9 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain-core, langchain-text-splitters, langchain-openai, langchain\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.1.53\n",
            "    Uninstalling langchain-core-0.1.53:\n",
            "      Successfully uninstalled langchain-core-0.1.53\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.0.2\n",
            "    Uninstalling langchain-text-splitters-0.0.2:\n",
            "      Successfully uninstalled langchain-text-splitters-0.0.2\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.1.10\n",
            "    Uninstalling langchain-0.1.10:\n",
            "      Successfully uninstalled langchain-0.1.10\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-community 0.0.27 requires langchain-core<0.2.0,>=0.1.30, but you have langchain-core 0.3.45 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-0.3.20 langchain-core-0.3.45 langchain-openai-0.3.8 langchain-text-splitters-0.3.6 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain langchain-openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kag0vRQUko9u",
        "outputId": "74ed99d8-7bdf-4033-dac7-2d8921afadac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWZ6O_eRmF5m",
        "outputId": "e40620bf-d7c2-4d60-d957-17a58c2d0b1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install groq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM-o8ncjoTOz",
        "outputId": "7c34d052-d528-47d1-c92f-60bcceb49d69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-core==0.1.37\n",
            "  Downloading langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.1.37) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.1.37) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.1.37) (0.1.147)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.1.37) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.1.37) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.1.37) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.1.37) (8.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.1.37) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core==0.1.37) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core==0.1.37) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core==0.1.37) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core==0.1.37) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core==0.1.37) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core==0.1.37) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-core==0.1.37) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-core==0.1.37) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-core==0.1.37) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-core==0.1.37) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.1.37) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.1.37) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.1.37) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.1.37) (1.3.1)\n",
            "Downloading langchain_core-0.1.37-py3-none-any.whl (274 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/274.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.6/274.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.1.53\n",
            "    Uninstalling langchain-core-0.1.53:\n",
            "      Successfully uninstalled langchain-core-0.1.53\n",
            "Successfully installed langchain-core-0.1.37\n",
            "Collecting langchain-groq==0.1.1\n",
            "  Downloading langchain_groq-0.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from langchain-groq==0.1.1) (0.19.0)\n",
            "Collecting langchain-core<0.2.0,>=0.1.41 (from langchain-groq==0.1.1)\n",
            "  Using cached langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.1) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.1) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.1) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.1) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.1) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.1) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (0.1.147)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (23.2)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (8.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq==0.1.1) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq==0.1.1) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq==0.1.1) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq==0.1.1) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (3.10.15)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq==0.1.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq==0.1.1) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (2.3.0)\n",
            "Downloading langchain_groq-0.1.1-py3-none-any.whl (11 kB)\n",
            "Using cached langchain_core-0.1.53-py3-none-any.whl (303 kB)\n",
            "Installing collected packages: langchain-core, langchain-groq\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.1.37\n",
            "    Uninstalling langchain-core-0.1.37:\n",
            "      Successfully uninstalled langchain-core-0.1.37\n",
            "Successfully installed langchain-core-0.1.53 langchain-groq-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-core==0.1.37\n",
        "!pip install langchain-groq==0.1.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM2zAVzIXGxX",
        "outputId": "e6b82a1f-2c6e-4c6f-e89f-48d1c473f6fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "# Set GPU device\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "\n",
        "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
        "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2qt6JHCgMFf",
        "outputId": "8d3b0b6b-f7eb-4b60-ef54-66e4e69fcea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "11.7\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())  # Should print True\n",
        "print(torch.version.cuda)  # Check CUDA version\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stJ141lXX3Ig",
        "outputId": "ce64b62d-1a3f-429e-9d60-57198db7a352"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 6 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SOE HTET NAING \n",
            "CONTACT INFORMATION Phone: +959965723434 Email: soehtetnaing.97@gmail.com Location: Yangon, Myanmar Date of Birth: 15th July 1997 PROFESSIONAL SUMMARY Motivated and results-driven professional with 5 years of experience across IT, management, and entertainment industries. Adept at business development, data analysis, customer relationship management, and AI-driven decision-making. Currently pursuing a Master of Science in Data Science and AI at AIT, Thailand to deepen expertise in machine learning, artificial intelligence, and analytics. Passionate about leveraging technology to shape the future while integrating cultural values to ensure ethical advancements. EDUCATION Master of Science in Data Science and AI (Ongoing) Asian Institute of Technology (AIT), Thailand Bachelor of Computer Science (Business Information Systems) (2013-2018) University of Information Technology, Yangon ICM Single Subject Diploma in Business Management & Administration (BMA) (2019) Strategy First University, Yangon Basic Education High School (2012-2013) No(5), Basic Education High School, Bothahtaung, Yangon, Myanmar WORK EXPERIENCE General Manager USM Cosmetic Distribution, Yangon (May 2019 - Feb 2024) - Managed customer and supplier relationships. - Led purchasing, price negotiation, budgeting, and financial calculations. - Analyzed stock and customer order data. - Implemented efficient business strategies. \n",
            "Freelance Translator & Video Encoder TSF Entertainment, Yangon (Jul 2020 - Oct 2022) - Translated and proofread English subtitles to Myanmar. - Synchronized subtitles and performed quality assurance. - Managed video editing and subtitle styling. Business Development Executive Global Wave Technology, Yangon (Jun 2018 - Apr 2019) - Developed marketing strategies. - Managed cross-functional collaborations and business proposals. - Conducted data analysis and reporting. SKILLS Technical Skills: Data Analysis, AI & Machine Learning, Python, Microsoft Office Soft Skills: Communication, Analytical Thinking, Time Management, Adaptability, Collaboration Languages: Burmese (Native), English (Fluent) RESEARCH INTERESTS & ACADEMIC GOALS - Investigating biases in financial language models and their societal impact. - Exploring explainable AI (XAI) and ethical AI practices in real-world applications. - Developing data-driven solutions for business intelligence and decision-making. - Enhancing stereo depth estimation models for improved computer vision applications. PERSONAL PERSPECTIVES Role of Technology in Society: Technology plays a critical role in economic growth, healthcare, education, and business operations. However, it must be aligned with ethical considerations to minimize biases and ensure responsible AI deployment. Cultural Values & Technological Advancements: Cultural values should serve as a guiding framework for AI and technological development, ensuring inclusivity and respect for diverse perspectives. Challenges as a Master's Student: - Balancing advanced coursework, research, and industry applications. - Staying updated with rapid advancements in AI and data science. - Adapting to new interdisciplinary methodologies in data-driven decision-making. \n",
            " \n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load the PDF file\n",
        "pdf_path = \"/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf\"  # Adjust path if needed\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "# Display extracted text for verification\n",
        "for doc in documents[:3]:  # Display first 3 extracted text parts\n",
        "    print(doc.page_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SnycUJZYW8p",
        "outputId": "c9fb5360-cdd5-485b-a773-5094a1aba812"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycI-_1S0Yc6n",
        "outputId": "464493d1-7b47-4666-dc72-a23d251d927a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content=\"Freelance Translator & Video Encoder TSF Entertainment, Yangon (Jul 2020 - Oct 2022) - Translated and proofread English subtitles to Myanmar. - Synchronized subtitles and performed quality assurance. - Managed video editing and subtitle styling. Business Development Executive Global Wave Technology, Yangon (Jun 2018 - Apr 2019) - Developed marketing strategies. - Managed cross-functional collaborations and business proposals. - Conducted data analysis and reporting. SKILLS Technical Skills: Data Analysis, AI & Machine Learning, Python, Microsoft Office Soft Skills: Communication, Analytical Thinking, Time Management, Adaptability, Collaboration Languages: Burmese (Native), English (Fluent) RESEARCH INTERESTS & ACADEMIC GOALS - Investigating biases in financial language models and their societal impact. - Exploring explainable AI (XAI) and ethical AI practices in real-world applications. - Developing data-driven solutions for business intelligence and decision-making. - Enhancing stereo depth estimation models for improved computer vision applications. PERSONAL PERSPECTIVES Role of Technology in Society: Technology plays a critical role in economic growth, healthcare, education, and business operations. However, it must be aligned with ethical considerations to minimize biases and ensure responsible AI deployment. Cultural Values & Technological Advancements: Cultural values should serve as a guiding framework for AI and technological development, ensuring inclusivity and respect for diverse perspectives. Challenges as a Master's Student: - Balancing advanced coursework, research, and industry applications. - Staying updated with rapid advancements in AI and data science. - Adapting to new interdisciplinary methodologies in data-driven decision-making. \", metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJpDMfJmYD_N",
        "outputId": "1cd558cd-4f11-4a3b-de14-4212ec8073d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Chunks: 79\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Split the documents into smaller chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=50,  # Chunk size of 50 characters\n",
        "    chunk_overlap= 10  # 10-character overlap\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"Total Chunks: {len(chunks)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfePq7mVYPxi",
        "outputId": "ecbccdef-03ef-4c26-a760-d53445a8c7c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import trange\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "\n",
        "model_name = 'hkunlp/instructor-base'\n",
        "\n",
        "embedding_model = HuggingFaceInstructEmbeddings(\n",
        "    model_name = model_name,\n",
        "    model_kwargs = {\"device\" : device}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "eI7hICxTYjr1"
      },
      "outputs": [],
      "source": [
        "#save vector locally\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "\n",
        "vector_path = '/content/drive/MyDrive/Colab_Notebooks/vector-store'\n",
        "\n",
        "vectordb = FAISS.from_documents(\n",
        "    documents = chunks,\n",
        "    embedding = embedding_model\n",
        ")\n",
        "\n",
        "db_file_name = 'Soe'\n",
        "\n",
        "vectordb.save_local(\n",
        "    folder_path = os.path.join(vector_path, db_file_name),\n",
        "    index_name = 'SHN' #default index\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlVfeNe1Y_3h",
        "outputId": "6fba89dd-9cf9-4f14-c258-861a34ca504a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ FAISS index loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#calling vector from local\n",
        "vector_path = '/content/drive/MyDrive/Colab_Notebooks/vector-store'\n",
        "db_file_name = 'Soe'\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "vectordb = FAISS.load_local(\n",
        "    folder_path=os.path.join(vector_path, db_file_name),\n",
        "    embeddings=embedding_model,\n",
        "    index_name='SHN',\n",
        "    allow_dangerous_deserialization=True  # ✅ Enable safe deserialization\n",
        ")\n",
        "\n",
        "print(\"✅ FAISS index loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4Nko15XAZpwC"
      },
      "outputs": [],
      "source": [
        "#ready to use\n",
        "retriever = vectordb.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fXj0CH0ZhRc",
        "outputId": "8d018c02-7c39-446e-d1bb-5a900f198a9a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(page_content='EDUCATION Master of Science in Data Science and', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}),\n",
              " Document(page_content='Yangon Basic Education High School (2012-2013)', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}),\n",
              " Document(page_content='No(5), Basic Education High School, Bothahtaung,', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}),\n",
              " Document(page_content='role in economic growth, healthcare, education,', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1})]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "retriever.get_relevant_documents(\"education?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDYy__bmZnks",
        "outputId": "1911d2ba-8528-4928-ce28-6a24dccf402d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='Languages: Burmese (Native), English (Fluent)', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}),\n",
              " Document(page_content='financial language models and their societal', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}),\n",
              " Document(page_content='subtitles and performed quality assurance. -', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}),\n",
              " Document(page_content='English subtitles to Myanmar. - Synchronized', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1})]"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "retriever.get_relevant_documents(\"language?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "nLSLS0CGZw0n"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "Please answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Gentle & Informative Answer:\n",
        "\"\"\".strip()\n",
        "\n",
        "# Load Prompt Template\n",
        "PROMPT = PromptTemplate.from_template(prompt_template)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TIKT9mhHg1Vr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utwqoAMRChkH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Xm4m0JVaE2H",
        "outputId": "68047585-23fe-4432-b230-39cf8f09ca39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
        "from langchain.llms.base import LLM\n",
        "from pydantic.v1 import BaseModel, Extra\n",
        "import torch\n",
        "\n",
        "# Model setup\n",
        "model_id = '/content/drive/MyDrive/Colab_Notebooks/A6_models/fastchat-t5-3b-v1.0/'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "bitsandbyte_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bitsandbyte_config,\n",
        "    device_map='auto',\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    task=\"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0,\n",
        "    repetition_penalty=1.5\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "wJmGn-dxEscS"
      },
      "outputs": [],
      "source": [
        "class CustomHuggingFacePipeline(LLM, BaseModel):\n",
        "    pipeline: object\n",
        "\n",
        "    class Config:\n",
        "        extra = Extra.allow\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def _call(self, prompt: str, stop=None, **kwargs):\n",
        "        response = self.pipeline(prompt, max_new_tokens=256, temperature=0, repetition_penalty=1.5)\n",
        "        return response[0][\"generated_text\"]\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self):\n",
        "        return \"custom_transformers\"\n",
        "\n",
        "llm = CustomHuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq1YkFQjEv4b"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain  # Import LLMChain\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "#  Question Generator (Rewrites User Queries)\n",
        "question_generator = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=CONDENSE_QUESTION_PROMPT,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "#  Retrieval Chain (Fetches Relevant Documents)\n",
        "doc_chain = load_qa_chain(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    prompt=PROMPT,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "#  Conversational Memory\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    k=3,\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    output_key=\"answer\"\n",
        ")\n",
        "\n",
        "#  Conversational Retrieval Chain\n",
        "chain = ConversationalRetrievalChain(\n",
        "    retriever=retriever,  # FAISS Retriever\n",
        "    question_generator=question_generator,\n",
        "    combine_docs_chain=doc_chain,\n",
        "    return_source_documents=True,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3_V0fekNcXfS"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"How old are you?\",\n",
        "    \"What is your highest level of education?\",\n",
        "    \"What major or field of study did you pursue during your education?\",\n",
        "    \"How many years of work experience do you have?\",\n",
        "    \"What type of work or industry have you been involved in?\",\n",
        "    \"Can you describe your current role or job responsibilities?\",\n",
        "    \"What are your core beliefs regarding the role of technology in shaping society?\",\n",
        "    \"How do you think cultural values should influence technological advancements?\",\n",
        "    \"As a master’s student, what is the most challenging aspect of your studies so far?\",\n",
        "    \"What specific research interests or academic goals do you hope to achieve during your time as a master’s student?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxcTxP_VNsRO",
        "outputId": "07d29fd2-ad47-475a-d849-a67401afbd0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "and results-driven professional with 5 years of\n",
            "\n",
            "Location: Yangon, Myanmar Date of Birth: 15th\n",
            "\n",
            "15th July 1997 PROFESSIONAL SUMMARY Motivated and\n",
            "\n",
            "Email: soehtetnaing.97@gmail.com Location:\n",
            "\n",
            "Question: How old are you?\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'How old are you?': {'question': 'How old are you?', 'chat_history': [], 'answer': '<pad> Soe  Htet  Naing  is  20  years  old.\\n', 'source_documents': [Document(page_content='and results-driven professional with 5 years of', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='Location: Yangon, Myanmar Date of Birth: 15th', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='15th July 1997 PROFESSIONAL SUMMARY Motivated and', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='Email: soehtetnaing.97@gmail.com Location:', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: How old are you?\n",
            "Assistant: <pad> Soe  Htet  Naing  is  20  years  old.\n",
            "\n",
            "Follow Up Input: What is your highest level of education?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "EDUCATION Master of Science in Data Science and\n",
            "\n",
            "No(5), Basic Education High School, Bothahtaung,\n",
            "\n",
            "Yangon Basic Education High School (2012-2013)\n",
            "\n",
            "a Master of Science in Data Science and AI at\n",
            "\n",
            "Question: <pad> What  is  your  highest  level  of  education?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'What is your highest level of education?': {'question': 'What is your highest level of education?', 'chat_history': [HumanMessage(content='How old are you?'), AIMessage(content='<pad> Soe  Htet  Naing  is  20  years  old.\\n')], 'answer': \"<pad> Soe  Htet  Naing's  highest  level  of  education  is  a  Master  of  Science  in  Data  Science  and  AI.\\n\", 'source_documents': [Document(page_content='EDUCATION Master of Science in Data Science and', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='No(5), Basic Education High School, Bothahtaung,', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='Yangon Basic Education High School (2012-2013)', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='a Master of Science in Data Science and AI at', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: How old are you?\n",
            "Assistant: <pad> Soe  Htet  Naing  is  20  years  old.\n",
            "\n",
            "Human: What is your highest level of education?\n",
            "Assistant: <pad> Soe  Htet  Naing's  highest  level  of  education  is  a  Master  of  Science  in  Data  Science  and  AI.\n",
            "\n",
            "Follow Up Input: What major or field of study did you pursue during your education?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "EDUCATION Master of Science in Data Science and\n",
            "\n",
            "advanced coursework, research, and industry\n",
            "\n",
            "15th July 1997 PROFESSIONAL SUMMARY Motivated and\n",
            "\n",
            "Systems) (2013-2018) University of Information\n",
            "\n",
            "Question: <pad> What  was  your  major  or  field  of  study  during  your  education?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'What major or field of study did you pursue during your education?': {'question': 'What major or field of study did you pursue during your education?', 'chat_history': [HumanMessage(content='How old are you?'), AIMessage(content='<pad> Soe  Htet  Naing  is  20  years  old.\\n'), HumanMessage(content='What is your highest level of education?'), AIMessage(content=\"<pad> Soe  Htet  Naing's  highest  level  of  education  is  a  Master  of  Science  in  Data  Science  and  AI.\\n\")], 'answer': \"<pad> Soe  Htet  Naing's  major  or  field  of  study  during  his  education  was  Data  Science.\\n\", 'source_documents': [Document(page_content='EDUCATION Master of Science in Data Science and', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='advanced coursework, research, and industry', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='15th July 1997 PROFESSIONAL SUMMARY Motivated and', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='Systems) (2013-2018) University of Information', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: How old are you?\n",
            "Assistant: <pad> Soe  Htet  Naing  is  20  years  old.\n",
            "\n",
            "Human: What is your highest level of education?\n",
            "Assistant: <pad> Soe  Htet  Naing's  highest  level  of  education  is  a  Master  of  Science  in  Data  Science  and  AI.\n",
            "\n",
            "Human: What major or field of study did you pursue during your education?\n",
            "Assistant: <pad> Soe  Htet  Naing's  major  or  field  of  study  during  his  education  was  Data  Science.\n",
            "\n",
            "Follow Up Input: How many years of work experience do you have?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "years of experience across IT, management, and\n",
            "\n",
            "and results-driven professional with 5 years of\n",
            "\n",
            "Yangon, Myanmar WORK EXPERIENCE General Manager\n",
            "\n",
            "Skills: Communication, Analytical Thinking, Time\n",
            "\n",
            "Question: <pad>  How  many  years  of  work  experience  do  you  have?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'How many years of work experience do you have?': {'question': 'How many years of work experience do you have?', 'chat_history': [HumanMessage(content='How old are you?'), AIMessage(content='<pad> Soe  Htet  Naing  is  20  years  old.\\n'), HumanMessage(content='What is your highest level of education?'), AIMessage(content=\"<pad> Soe  Htet  Naing's  highest  level  of  education  is  a  Master  of  Science  in  Data  Science  and  AI.\\n\"), HumanMessage(content='What major or field of study did you pursue during your education?'), AIMessage(content=\"<pad> Soe  Htet  Naing's  major  or  field  of  study  during  his  education  was  Data  Science.\\n\")], 'answer': '<pad> Soe  Htet  Naing  has  5  years  of  work  experience.\\n', 'source_documents': [Document(page_content='years of experience across IT, management, and', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='and results-driven professional with 5 years of', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='Yangon, Myanmar WORK EXPERIENCE General Manager', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='Skills: Communication, Analytical Thinking, Time', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What is your highest level of education?\n",
            "Assistant: <pad> Soe  Htet  Naing's  highest  level  of  education  is  a  Master  of  Science  in  Data  Science  and  AI.\n",
            "\n",
            "Human: What major or field of study did you pursue during your education?\n",
            "Assistant: <pad> Soe  Htet  Naing's  major  or  field  of  study  during  his  education  was  Data  Science.\n",
            "\n",
            "Human: How many years of work experience do you have?\n",
            "Assistant: <pad> Soe  Htet  Naing  has  5  years  of  work  experience.\n",
            "\n",
            "Follow Up Input: What type of work or industry have you been involved in?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "years of experience across IT, management, and\n",
            "\n",
            "and entertainment industries. Adept at business\n",
            "\n",
            "industry applications. - Staying updated with\n",
            "\n",
            "advanced coursework, research, and industry\n",
            "\n",
            "Question: <pad>  What  type  of  work  or  industry  have  you  been  involved  in?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'What type of work or industry have you been involved in?': {'question': 'What type of work or industry have you been involved in?', 'chat_history': [HumanMessage(content='What is your highest level of education?'), AIMessage(content=\"<pad> Soe  Htet  Naing's  highest  level  of  education  is  a  Master  of  Science  in  Data  Science  and  AI.\\n\"), HumanMessage(content='What major or field of study did you pursue during your education?'), AIMessage(content=\"<pad> Soe  Htet  Naing's  major  or  field  of  study  during  his  education  was  Data  Science.\\n\"), HumanMessage(content='How many years of work experience do you have?'), AIMessage(content='<pad> Soe  Htet  Naing  has  5  years  of  work  experience.\\n')], 'answer': '<pad> Soe  Htet  Naing  has  been  involved  in  the  IT,  management,  and  entertainment  industries.\\n', 'source_documents': [Document(page_content='years of experience across IT, management, and', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='and entertainment industries. Adept at business', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='industry applications. - Staying updated with', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='advanced coursework, research, and industry', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What major or field of study did you pursue during your education?\n",
            "Assistant: <pad> Soe  Htet  Naing's  major  or  field  of  study  during  his  education  was  Data  Science.\n",
            "\n",
            "Human: How many years of work experience do you have?\n",
            "Assistant: <pad> Soe  Htet  Naing  has  5  years  of  work  experience.\n",
            "\n",
            "Human: What type of work or industry have you been involved in?\n",
            "Assistant: <pad> Soe  Htet  Naing  has  been  involved  in  the  IT,  management,  and  entertainment  industries.\n",
            "\n",
            "Follow Up Input: Can you describe your current role or job responsibilities?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "analysis and reporting. SKILLS Technical Skills:\n",
            "\n",
            "15th July 1997 PROFESSIONAL SUMMARY Motivated and\n",
            "\n",
            "industry applications. - Staying updated with\n",
            "\n",
            "PERSONAL PERSPECTIVES Role of Technology in\n",
            "\n",
            "Question: <pad>  What  is  your  current  role  or  job  responsibilities?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'Can you describe your current role or job responsibilities?': {'question': 'Can you describe your current role or job responsibilities?', 'chat_history': [HumanMessage(content='What major or field of study did you pursue during your education?'), AIMessage(content=\"<pad> Soe  Htet  Naing's  major  or  field  of  study  during  his  education  was  Data  Science.\\n\"), HumanMessage(content='How many years of work experience do you have?'), AIMessage(content='<pad> Soe  Htet  Naing  has  5  years  of  work  experience.\\n'), HumanMessage(content='What type of work or industry have you been involved in?'), AIMessage(content='<pad> Soe  Htet  Naing  has  been  involved  in  the  IT,  management,  and  entertainment  industries.\\n')], 'answer': \"<pad> Soe  Htet  Naing's  current  role  or  job  responsibilities  are:\\n *  Technical  Skills:  Analysis  and  reporting.\\n\", 'source_documents': [Document(page_content='analysis and reporting. SKILLS Technical Skills:', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='15th July 1997 PROFESSIONAL SUMMARY Motivated and', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='industry applications. - Staying updated with', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='PERSONAL PERSPECTIVES Role of Technology in', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: How many years of work experience do you have?\n",
            "Assistant: <pad> Soe  Htet  Naing  has  5  years  of  work  experience.\n",
            "\n",
            "Human: What type of work or industry have you been involved in?\n",
            "Assistant: <pad> Soe  Htet  Naing  has  been  involved  in  the  IT,  management,  and  entertainment  industries.\n",
            "\n",
            "Human: Can you describe your current role or job responsibilities?\n",
            "Assistant: <pad> Soe  Htet  Naing's  current  role  or  job  responsibilities  are:\n",
            " *  Technical  Skills:  Analysis  and  reporting.\n",
            "\n",
            "Follow Up Input: What are your core beliefs regarding the role of technology in shaping society?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "in Society: Technology plays a critical role in\n",
            "\n",
            "PERSONAL PERSPECTIVES Role of Technology in\n",
            "\n",
            "technology to shape the future while integrating\n",
            "\n",
            "societal impact. - Exploring explainable AI (XAI)\n",
            "\n",
            "Question: <pad> What  are  your  thoughts  on  the  potential  negative  effects  of  technology  on  society?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'What are your core beliefs regarding the role of technology in shaping society?': {'question': 'What are your core beliefs regarding the role of technology in shaping society?', 'chat_history': [HumanMessage(content='How many years of work experience do you have?'), AIMessage(content='<pad> Soe  Htet  Naing  has  5  years  of  work  experience.\\n'), HumanMessage(content='What type of work or industry have you been involved in?'), AIMessage(content='<pad> Soe  Htet  Naing  has  been  involved  in  the  IT,  management,  and  entertainment  industries.\\n'), HumanMessage(content='Can you describe your current role or job responsibilities?'), AIMessage(content=\"<pad> Soe  Htet  Naing's  current  role  or  job  responsibilities  are:\\n *  Technical  Skills:  Analysis  and  reporting.\\n\")], 'answer': '<pad> Soe  Htet  Naing  believes  that  technology  has  the  potential  to  have  both  positive  and  negative  effects  on  society.  However,  he  also  recognizes  the  importance  of  considering  these  impacts  when  developing  new  technologies  and  making  decisions  about  their  use.\\n', 'source_documents': [Document(page_content='in Society: Technology plays a critical role in', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='PERSONAL PERSPECTIVES Role of Technology in', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='technology to shape the future while integrating', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='societal impact. - Exploring explainable AI (XAI)', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What type of work or industry have you been involved in?\n",
            "Assistant: <pad> Soe  Htet  Naing  has  been  involved  in  the  IT,  management,  and  entertainment  industries.\n",
            "\n",
            "Human: Can you describe your current role or job responsibilities?\n",
            "Assistant: <pad> Soe  Htet  Naing's  current  role  or  job  responsibilities  are:\n",
            " *  Technical  Skills:  Analysis  and  reporting.\n",
            "\n",
            "Human: What are your core beliefs regarding the role of technology in shaping society?\n",
            "Assistant: <pad> Soe  Htet  Naing  believes  that  technology  has  the  potential  to  have  both  positive  and  negative  effects  on  society.  However,  he  also  recognizes  the  importance  of  considering  these  impacts  when  developing  new  technologies  and  making  decisions  about  their  use.\n",
            "\n",
            "Follow Up Input: How do you think cultural values should influence technological advancements?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "Advancements: Cultural values should serve as a\n",
            "\n",
            "cultural values to ensure ethical advancements.\n",
            "\n",
            "AI deployment. Cultural Values & Technological\n",
            "\n",
            "technology to shape the future while integrating\n",
            "\n",
            "Question: <pad> How  do  you  think  cultural  values  should  influence  technological  advancements?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'How do you think cultural values should influence technological advancements?': {'question': 'How do you think cultural values should influence technological advancements?', 'chat_history': [HumanMessage(content='What type of work or industry have you been involved in?'), AIMessage(content='<pad> Soe  Htet  Naing  has  been  involved  in  the  IT,  management,  and  entertainment  industries.\\n'), HumanMessage(content='Can you describe your current role or job responsibilities?'), AIMessage(content=\"<pad> Soe  Htet  Naing's  current  role  or  job  responsibilities  are:\\n *  Technical  Skills:  Analysis  and  reporting.\\n\"), HumanMessage(content='What are your core beliefs regarding the role of technology in shaping society?'), AIMessage(content='<pad> Soe  Htet  Naing  believes  that  technology  has  the  potential  to  have  both  positive  and  negative  effects  on  society.  However,  he  also  recognizes  the  importance  of  considering  these  impacts  when  developing  new  technologies  and  making  decisions  about  their  use.\\n')], 'answer': '<pad> Cultural  values  should  serve  as  a  guideline  for  ethical  advancements  in  technological  development,  including  AI  deployment.  This  means  that  cultural  values  should  be  integrated  into  the  design  and  implementation  of  technology  to  ensure  that  it  is  aligned  with  ethical  principles  and  practices.\\n', 'source_documents': [Document(page_content='Advancements: Cultural values should serve as a', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='cultural values to ensure ethical advancements.', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='AI deployment. Cultural Values & Technological', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='technology to shape the future while integrating', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Can you describe your current role or job responsibilities?\n",
            "Assistant: <pad> Soe  Htet  Naing's  current  role  or  job  responsibilities  are:\n",
            " *  Technical  Skills:  Analysis  and  reporting.\n",
            "\n",
            "Human: What are your core beliefs regarding the role of technology in shaping society?\n",
            "Assistant: <pad> Soe  Htet  Naing  believes  that  technology  has  the  potential  to  have  both  positive  and  negative  effects  on  society.  However,  he  also  recognizes  the  importance  of  considering  these  impacts  when  developing  new  technologies  and  making  decisions  about  their  use.\n",
            "\n",
            "Human: How do you think cultural values should influence technological advancements?\n",
            "Assistant: <pad> Cultural  values  should  serve  as  a  guideline  for  ethical  advancements  in  technological  development,  including  AI  deployment.  This  means  that  cultural  values  should  be  integrated  into  the  design  and  implementation  of  technology  to  ensure  that  it  is  aligned  with  ethical  principles  and  practices.\n",
            "\n",
            "Follow Up Input: As a master’s student, what is the most challenging aspect of your studies so far?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "for diverse perspectives. Challenges as a\n",
            "\n",
            "(Fluent) RESEARCH INTERESTS & ACADEMIC GOALS -\n",
            "\n",
            "advanced coursework, research, and industry\n",
            "\n",
            "- Adapting to new interdisciplinary methodologies\n",
            "\n",
            "Question: <pad>  What  has  been  the  most  challenging  aspect  of  your  studies  so  far?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'As a master’s student, what is the most challenging aspect of your studies so far?': {'question': 'As a master’s student, what is the most challenging aspect of your studies so far?', 'chat_history': [HumanMessage(content='Can you describe your current role or job responsibilities?'), AIMessage(content=\"<pad> Soe  Htet  Naing's  current  role  or  job  responsibilities  are:\\n *  Technical  Skills:  Analysis  and  reporting.\\n\"), HumanMessage(content='What are your core beliefs regarding the role of technology in shaping society?'), AIMessage(content='<pad> Soe  Htet  Naing  believes  that  technology  has  the  potential  to  have  both  positive  and  negative  effects  on  society.  However,  he  also  recognizes  the  importance  of  considering  these  impacts  when  developing  new  technologies  and  making  decisions  about  their  use.\\n'), HumanMessage(content='How do you think cultural values should influence technological advancements?'), AIMessage(content='<pad> Cultural  values  should  serve  as  a  guideline  for  ethical  advancements  in  technological  development,  including  AI  deployment.  This  means  that  cultural  values  should  be  integrated  into  the  design  and  implementation  of  technology  to  ensure  that  it  is  aligned  with  ethical  principles  and  practices.\\n')], 'answer': '<pad> Adapting  to  new  interdisciplinary  methodologies.\\n', 'source_documents': [Document(page_content='for diverse perspectives. Challenges as a', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='(Fluent) RESEARCH INTERESTS & ACADEMIC GOALS -', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='advanced coursework, research, and industry', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='- Adapting to new interdisciplinary methodologies', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What are your core beliefs regarding the role of technology in shaping society?\n",
            "Assistant: <pad> Soe  Htet  Naing  believes  that  technology  has  the  potential  to  have  both  positive  and  negative  effects  on  society.  However,  he  also  recognizes  the  importance  of  considering  these  impacts  when  developing  new  technologies  and  making  decisions  about  their  use.\n",
            "\n",
            "Human: How do you think cultural values should influence technological advancements?\n",
            "Assistant: <pad> Cultural  values  should  serve  as  a  guideline  for  ethical  advancements  in  technological  development,  including  AI  deployment.  This  means  that  cultural  values  should  be  integrated  into  the  design  and  implementation  of  technology  to  ensure  that  it  is  aligned  with  ethical  principles  and  practices.\n",
            "\n",
            "Human: As a master’s student, what is the most challenging aspect of your studies so far?\n",
            "Assistant: <pad> Adapting  to  new  interdisciplinary  methodologies.\n",
            "\n",
            "Follow Up Input: What specific research interests or academic goals do you hope to achieve during your time as a master’s student?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "as a Master's Student: - Balancing advanced\n",
            "\n",
            "EDUCATION Master of Science in Data Science and\n",
            "\n",
            "(Fluent) RESEARCH INTERESTS & ACADEMIC GOALS -\n",
            "\n",
            "a Master of Science in Data Science and AI at\n",
            "\n",
            "Question: <pad>  What  are  your  long-term  academic  and  research  goals  as  a  master’s  student?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'What specific research interests or academic goals do you hope to achieve during your time as a master’s student?': {'question': 'What specific research interests or academic goals do you hope to achieve during your time as a master’s student?', 'chat_history': [HumanMessage(content='What are your core beliefs regarding the role of technology in shaping society?'), AIMessage(content='<pad> Soe  Htet  Naing  believes  that  technology  has  the  potential  to  have  both  positive  and  negative  effects  on  society.  However,  he  also  recognizes  the  importance  of  considering  these  impacts  when  developing  new  technologies  and  making  decisions  about  their  use.\\n'), HumanMessage(content='How do you think cultural values should influence technological advancements?'), AIMessage(content='<pad> Cultural  values  should  serve  as  a  guideline  for  ethical  advancements  in  technological  development,  including  AI  deployment.  This  means  that  cultural  values  should  be  integrated  into  the  design  and  implementation  of  technology  to  ensure  that  it  is  aligned  with  ethical  principles  and  practices.\\n'), HumanMessage(content='As a master’s student, what is the most challenging aspect of your studies so far?'), AIMessage(content='<pad> Adapting  to  new  interdisciplinary  methodologies.\\n')], 'answer': \"<pad> As  a  Master's  student,  Soe  Htet  Naing's  long-term  academic  and  research  goals  as  a  data  science  and  AI  researcher  are:\\n 1.  Develop  advanced  skills  in  data  analysis  and  visualization,  including  machine  learning  algorithms  and  natural  language  processing.\\n 2.  Conduct  original  research  on  complex  problems  related  to  data  science  and  artificial  intelligence,  such  as  the  use  of  deep  learning  for  image  recognition  or  the  development  of  predictive  models  for  healthcare  applications.\\n 3.  Contribute  to  ongoing  scientific  discussions  and  publications  in  the  field  of  data  science  and  AI,  with  a  focus  on  developing  innovative  solutions  that  address  real-world  challenges.\\n 4.  Collaborate  with  other  researchers  and  practitioners  in  the  fields  of  data  science  and  AI  to  advance \", 'source_documents': [Document(page_content=\"as a Master's Student: - Balancing advanced\", metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='EDUCATION Master of Science in Data Science and', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='(Fluent) RESEARCH INTERESTS & ACADEMIC GOALS -', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='a Master of Science in Data Science and AI at', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0})]}\n"
          ]
        }
      ],
      "source": [
        "answers = []\n",
        "for question in questions:\n",
        "    answer = chain({\"question\": question})\n",
        "    answers.append(answer)\n",
        "    print(f\"Bot Answer for '{question}':\", answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "850pam2feFS8",
        "outputId": "f60b93f6-cba6-4d61-c4db-ec804f870fd6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> Soe  Htet  Naing's  highest  education  is  a  Master  of  Science  in  Data  Science  and  Analytics.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Function to generate responses\n",
        "def generate_response(question):\n",
        "    input_document = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\".join([doc.page_content for doc in input_document])\n",
        "\n",
        "    prompt = PROMPT.format(context=context, question=question)\n",
        "    response = pipe(prompt, max_new_tokens=150)\n",
        "\n",
        "    return response[0][\"generated_text\"]\n",
        "\n",
        "# Test response\n",
        "print(generate_response(\"What is Soe Htet Naing's highest education?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRp-DyrcjwSb",
        "outputId": "d7733c7a-b07c-4122-8351-8cc6c5276698"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> Soe  Htet  Naing's  native  language  is  Burmese.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test response\n",
        "print(generate_response(\"What is Soe Htet Naing's native language?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHVvrrQ9j3fv",
        "outputId": "3fa472be-caa8-4b58-b21e-4a017df2d722"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> No,  Soe  Htet  Naing's  skills  are  Communication,  Analytical  Thinking,  and  Time.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test response\n",
        "print(generate_response(\"What are Soe Htet Naing's skills?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGuJh67JkmDw",
        "outputId": "495877de-ac03-4b29-8581-2733b72c164f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available OpenAI Models:\n",
            "gpt-4o-mini-audio-preview-2024-12-17\n",
            "dall-e-3\n",
            "dall-e-2\n",
            "gpt-4o-audio-preview-2024-10-01\n",
            "gpt-4o-audio-preview\n",
            "o1-mini-2024-09-12\n",
            "o1-mini\n",
            "omni-moderation-latest\n",
            "gpt-4o-mini-audio-preview\n",
            "omni-moderation-2024-09-26\n",
            "babbage-002\n",
            "tts-1-hd-1106\n",
            "text-embedding-3-large\n",
            "gpt-4o-mini-2024-07-18\n",
            "gpt-4o-2024-11-20\n",
            "gpt-4o-2024-05-13\n",
            "tts-1-hd\n",
            "o1-preview\n",
            "o1-preview-2024-09-12\n",
            "gpt-4o-mini\n",
            "gpt-3.5-turbo-instruct-0914\n",
            "gpt-4o-mini-search-preview\n",
            "tts-1-1106\n",
            "davinci-002\n",
            "gpt-3.5-turbo-1106\n",
            "gpt-4o-search-preview\n",
            "gpt-3.5-turbo-instruct\n",
            "gpt-4o-mini-search-preview-2025-03-11\n",
            "gpt-3.5-turbo-0125\n",
            "gpt-4o-2024-08-06\n",
            "gpt-3.5-turbo\n",
            "gpt-3.5-turbo-16k\n",
            "gpt-4o\n",
            "text-embedding-3-small\n",
            "text-embedding-ada-002\n",
            "gpt-4.5-preview\n",
            "gpt-4.5-preview-2025-02-27\n",
            "gpt-4o-search-preview-2025-03-11\n",
            "tts-1\n",
            "whisper-1\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = \"your key\"\n",
        "\n",
        "# Create OpenAI client with API key\n",
        "client = openai.OpenAI(api_key=openai.api_key)\n",
        "\n",
        "# Fetch available models\n",
        "response = client.models.list()\n",
        "\n",
        "print(\"Available OpenAI Models:\")\n",
        "for model in response.data:\n",
        "    print(model.id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "taQl6BGCkFPY",
        "outputId": "1a36e68b-5181-405f-82d5-fcf95de649e2"
      },
      "outputs": [
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-be721207a656>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Test the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopenai_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Where did Phue Pwint Thwe study?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-be721207a656>\u001b[0m in \u001b[0;36mopenai_response\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPROMPT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopenai_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Test the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m         | `astream`                 | '''                                                            | AsyncIterator[BaseMessageChunk]                                     | Defaults to yielding output of ainvoke.                                                          |\n\u001b[1;32m    157\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34...\n\u001b[0;32m--> 158\u001b[0;31m         | `astream_events`          | '''                                                            | AsyncIterator[StreamEvent]                                          | Event types: 'on_chat_model_start', 'on_chat_model_stream', 'on_chat_model_end'.                 |\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[...\n\u001b[1;32m    160\u001b[0m         | `batch`                   | List[''']                                                      | List[BaseMessage]                                                   | Defaults to running invoke in concurrent threads.                                                |\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m     def _get_ls_params(\n\u001b[1;32m    559\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     ) -> LangSmithParams:\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             )\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0mgeneration\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatGenerationChunk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0minheritable_metadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m             )\n\u001b[1;32m    413\u001b[0m             (run_manager,) = callback_manager.on_chat_model_start(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;36m2.\u001b[0m \u001b[0mneed\u001b[0m \u001b[0mmore\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mjust\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtop\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;36m3.\u001b[0m \u001b[0mare\u001b[0m \u001b[0mbuilding\u001b[0m \u001b[0mchains\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mare\u001b[0m \u001b[0magnostic\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0munderlying\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 \u001b[0mtype\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpure\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mcompletion\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0mvs\u001b[0m \u001b[0mchat\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/openai.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         }\n\u001b[0;32m--> 441\u001b[0;31m         response = self.completion_with_retry(\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/openai.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;34m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_openai_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mretry_decorator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_retry_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    861\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    862\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         )\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    961\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1050\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1050\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "import os\n",
        "\n",
        "# Set API Key (Replace 'your-api-key' with your actual OpenAI API key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your key\"\n",
        "\n",
        "# Load OpenAI GPT Model\n",
        "openai_model = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.7, openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "def openai_response(question):\n",
        "    input_document = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\".join([doc.page_content for doc in input_document])\n",
        "\n",
        "    prompt = PROMPT.format(context=context, question=question)\n",
        "    return openai_model.invoke(prompt)\n",
        "\n",
        "# Test the model\n",
        "print(openai_response(\"Where did Soe Htet Naing study?\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glQ9nlBtkOF6"
      },
      "outputs": [],
      "source": [
        "your key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C2exr9Yq5yA"
      },
      "outputs": [],
      "source": [
        "# Set Groq API Key (replace with your key)\n",
        "os.environ[\"GROQ_API_KEY\"] = \"your key\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wivWPCovq3Hm",
        "outputId": "e847cffc-c3df-4aee-b779-c2c3e71d917f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('data', [Model(id='allam-2-7b', created=1737672203, object='model', owned_by='SDAIA', active=True, context_window=4096, public_apps=None), Model(id='mistral-saba-24b', created=1739996492, object='model', owned_by='Mistral AI', active=True, context_window=32768, public_apps=None), Model(id='llama-guard-3-8b', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='mixtral-8x7b-32768', created=1693721698, object='model', owned_by='Mistral AI', active=True, context_window=32768, public_apps=None), Model(id='deepseek-r1-distill-llama-70b', created=1737924940, object='model', owned_by='DeepSeek / Meta', active=True, context_window=131072, public_apps=None), Model(id='deepseek-r1-distill-qwen-32b', created=1738891590, object='model', owned_by='DeepSeek / Alibaba Cloud', active=True, context_window=131072, public_apps=None), Model(id='llama-3.1-8b-instant', created=1693721698, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None), Model(id='llama3-8b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-3.3-70b-specdec', created=1733505017, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-3.2-11b-vision-preview', created=1727226869, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='distil-whisper-large-v3-en', created=1693721698, object='model', owned_by='Hugging Face', active=True, context_window=448, public_apps=None), Model(id='gemma2-9b-it', created=1693721698, object='model', owned_by='Google', active=True, context_window=8192, public_apps=None), Model(id='whisper-large-v3', created=1693721698, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None), Model(id='qwen-2.5-32b', created=1738789898, object='model', owned_by='Alibaba Cloud', active=True, context_window=131072, public_apps=None), Model(id='qwen-qwq-32b', created=1741214760, object='model', owned_by='Alibaba Cloud', active=True, context_window=131072, public_apps=None), Model(id='llama-3.2-1b-preview', created=1727224268, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='qwen-2.5-coder-32b', created=1739494572, object='model', owned_by='Alibaba Cloud', active=True, context_window=131072, public_apps=None), Model(id='llama-3.2-90b-vision-preview', created=1727226914, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-3.2-3b-preview', created=1727224290, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama3-70b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='whisper-large-v3-turbo', created=1728413088, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None), Model(id='llama-3.3-70b-versatile', created=1733447754, object='model', owned_by='Meta', active=True, context_window=32768, public_apps=None)])\n",
            "('object', 'list')\n"
          ]
        }
      ],
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "models = client.models.list()\n",
        "\n",
        "for model in models:\n",
        "    print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KKk4yo8mnril"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "import os\n",
        "\n",
        "\n",
        "# Load Groq Model (Use `llama3-70b` or `mixtral-8x7b`)\n",
        "groq_model = ChatGroq(model_name=\"allam-2-7b\", temperature=0.7)\n",
        "\n",
        "def groq_response(question):\n",
        "    input_document = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\".join([doc.page_content for doc in input_document])\n",
        "\n",
        "    prompt = PROMPT.format(context=context, question=question)\n",
        "    return groq_model.invoke(prompt)\n",
        "\n",
        "# # Test the model\n",
        "# print(groq_response(\"Where did Soe Htet Naing study?\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzkYjcmhT9LU",
        "outputId": "86c6d2c2-3dca-4486-c2e3-0e67c4a0f6b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot Answer for 'How old are you?': content=\"To determine Soe Htet Naing's age, we need to calculate it based on the provided date of birth, which is July 15th, 1997. Since we are currently in the year, let's assume it is after 1997 but before 2004 (as there is no mention of the current year in the context).\\n\\nFirst, we subtract Soe Htet Naing's birth year from the current year:\\n2021 (current year) - 1997 (Soet Naing's birth year) = 24 years\\n\\nNext, we subtract Soe Htet Naing's birth month and day from the current month and day:\\nIf today's date is the 15th of the current month, then:\\n2021 - 1997 = 24 years\\n15 (current day) - 15 (Soet Naing's birth day) = 0 days difference\\n\\nTherefore, Soe Htet Naing is currently 24 years old, regardless of the exact day. \" response_metadata={'token_usage': {'completion_tokens': 246, 'prompt_tokens': 120, 'total_tokens': 366, 'completion_time': 0.17420818, 'prompt_time': 0.003858195, 'queue_time': 0.235385091, 'total_time': 0.178066375}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_1612a676d6', 'finish_reason': 'stop', 'logprobs': None} id='run-2637858c-3bba-4b58-8390-0df76248d414-0'\n",
            "Bot Answer for 'What is your highest level of education?': content='Based on the information provided in the context, Soe Htet Naing has attained the highest level of education in the form of a Master of Science in Data Science. This conclusion is drawn from the detailed educational background provided, which includes attending Yangon Basic Education High School from 2012 to 2013, participating in advanced coursework, research, and industry experiences. While these details contribute to his overall educational profile, the specific degree mentioned is the Master of Science in Data Science, which indicates the highest level of education achieved by Soe Htet Naing. ' response_metadata={'token_usage': {'completion_tokens': 120, 'prompt_tokens': 114, 'total_tokens': 234, 'completion_time': 0.084928581, 'prompt_time': 0.003589999, 'queue_time': 0.23518088699999998, 'total_time': 0.08851858}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_1612a676d6', 'finish_reason': 'stop', 'logprobs': None} id='run-c89dbb3f-9d5a-4021-8c62-0ac35c6e553b-0'\n",
            "Bot Answer for 'What major or field of study did you pursue during your education?': content='During your educational journey, you primarily focused on the field of Master of Science in Data Science and Artificial Intelligence. This emphasis on advanced coursework, research, and industry exposure allowed you to adapt proficiently to emerging interdisciplinary methodologies, demonstrating a keen interest and dedication to this field of study. ' response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 107, 'total_tokens': 172, 'completion_time': 0.046028585, 'prompt_time': 0.003451131, 'queue_time': 0.235221905, 'total_time': 0.049479716}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_1612a676d6', 'finish_reason': 'stop', 'logprobs': None} id='run-05e09b66-8422-41e3-9bf2-d418e85c38bf-0'\n",
            "Bot Answer for 'How many years of work experience do you have?': content=\"Based on the provided context, Soe Htet Naing possesses approximately 5 years of work experience. The details given highlight their expertise across IT, management, and results-driven professional roles, all within the city of Yangon, Myanmar. The General Manager position also indicates substantial experience in a leadership role. Therefore, when asked about Soe Htet Naing's work experience, we can confidently respond with '5 years' considering the rich history presented in the context. \" response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 106, 'total_tokens': 205, 'completion_time': 0.070080212, 'prompt_time': 0.003468492, 'queue_time': 0.469223441, 'total_time': 0.073548704}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_1612a676d6', 'finish_reason': 'stop', 'logprobs': None} id='run-5f2bfdef-fbfc-4c7a-8427-1f6e33c3b380-0'\n",
            "Bot Answer for 'What type of work or industry have you been involved in?': content=\"Based on the information provided about Soe Htet Naing, they have been actively engaged in various work and industries. They possess extensive experience in the IT, management, and entertainment sectors. Additionally, their proficiency in business-related applications and staying updated with advancements demonstrate their involvement in these domains. \\n\\nTo provide a more detailed response, let's break down Soe Htet Naing's expertise:\\n\\n1. IT: They have spent years honing their skills in this field, indicating involvement in the Information Technology industry. This could involve roles such as software development, system administration, or IT project management.\\n\\n2. Management: Alongside their IT background, Soe Htet Naing has also gained expertise in management. This could include roles like team leader, project manager, or even business consultant, where they contribute to organizational strategy and decision-making.\\n\\n3. Entertainment: Their involvement in the entertainment industry is evidenced by their proficiency in business applications. This could involve roles that encompass content creation, distribution, or management within the entertainment sector.\\n\\n4. Advanced coursework, research, and industry expertise: Soe Htet Naing's dedication to ongoing learning and industry updates demonstrates their commitment to staying relevant and competitive in their fields. This could include engaging in further education, conducting research, or actively participating in industry events and discussions.\\n\\nIn summary, Soe Htet Naing has been actively engaged in the Information Technology, management, and entertainment industries, demonstrating their expertise in these domains through their work experience, education, and ongoing industry involvement. \" response_metadata={'token_usage': {'completion_tokens': 332, 'prompt_tokens': 103, 'total_tokens': 435, 'completion_time': 0.234470767, 'prompt_time': 0.003457362, 'queue_time': 0.235075482, 'total_time': 0.237928129}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_1612a676d6', 'finish_reason': 'stop', 'logprobs': None} id='run-f432e674-54dc-4cca-b6cf-3023b438be14-0'\n",
            "Bot Answer for 'Can you describe your current role or job responsibilities?': content=\"Based on the provided context, Soe Htet Naing seems to be a knowledgeable individual with a strong focus on analysis and reporting. Their professional background highlights their motivation and dedication in the field. Their skillset includes technical skills such as staying updated with industry applications.\\n\\nHowever, the specific job responsibilities are not explicitly mentioned within the given context. Considering the individual's background, we can infer that their role likely involves utilizing their technical skills and staying informed about industry developments to analyze, report on, and potentially advise on various topics within a professional setting. Their willingness to be gentle and informative aligns with providing insightful analysis and recommendations. \" response_metadata={'token_usage': {'completion_tokens': 136, 'prompt_tokens': 119, 'total_tokens': 255, 'completion_time': 0.095089822, 'prompt_time': 0.003790306, 'queue_time': 0.43338828599999996, 'total_time': 0.098880128}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_77a9c76dc9', 'finish_reason': 'stop', 'logprobs': None} id='run-f4fcfc80-f361-4530-96ea-fe00c3f3559b-0'\n",
            "Bot Answer for 'What are your core beliefs regarding the role of technology in shaping society?': content='Based on the provided context about Soe Htet Naing, their core beliefs revolve around the essential role that technology plays in shaping society. They emphasize the significance of integrating technology into various aspects of life, including shaping the future and integrating it with personal perspectives on issues like AI deployment and cultural values. Therefore, my answer to your question would be:\\n\\nYour core beliefs seem to align with those of Soe Htet Naing, who advocates for the significant role of technology in shaping our society. They understand that technology is instrumental in molding our collective future and should be incorporated carefully, considering our cultural values alongside its advancements. ' response_metadata={'token_usage': {'completion_tokens': 134, 'prompt_tokens': 106, 'total_tokens': 240, 'completion_time': 0.093727938, 'prompt_time': 0.0034506, 'queue_time': 0.241668286, 'total_time': 0.097178538}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_77a9c76dc9', 'finish_reason': 'stop', 'logprobs': None} id='run-c6321417-9c85-40b8-8e04-3a56c7d69a3b-0'\n",
            "Bot Answer for 'How do you think cultural values should influence technological advancements?': content=\"In the context of Soe Htet Naing's perspective, I would explain how cultural values should influence technological advancements. According to Soe Htet Naing, cultural values should play a significant role in guiding the development and deployment of AI (Artificial Intelligence) technologies.\\n\\nCultural values, in this context, act as a fundamental framework that should be integrated with technological advancements, particularly AI. By doing so, we ensure that these innovations align with our ethical principles and maintain a balance between progress and preserving our core societal values.\\n\\nThe integration of cultural values ensures that AI technologies are developed and used in ways that respect human dignity, promote social harmony, and safeguard our collective wellbeing. This approach would help shape the future of technology, giving it a sense of purpose and ensuring that it contributes positively to society. \" response_metadata={'token_usage': {'completion_tokens': 174, 'prompt_tokens': 100, 'total_tokens': 274, 'completion_time': 0.121682832, 'prompt_time': 0.003293182, 'queue_time': 0.24143774399999998, 'total_time': 0.124976014}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_77a9c76dc9', 'finish_reason': 'stop', 'logprobs': None} id='run-e61bde97-f024-4261-bea8-1bdac2caf79a-0'\n",
            "Bot Answer for 'As a master’s student, what is the most challenging aspect of your studies so far?': content=\"Based on the provided context, the most challenging aspect of Soe Htet Naing's master's studies so far seems to be balancing the advanced coursework with the aim of acquiring diverse perspectives. This is inferred from the context that highlights the demanding nature of pursuing a Master of Science in Data Science and AI, which involves tackling advanced subjects. Therefore, the answer to the rephrased question would be:\\n\\nThe most challenging part of my studies as a master's student has been juggling the demanding coursework to gain diverse perspectives. \" response_metadata={'token_usage': {'completion_tokens': 116, 'prompt_tokens': 112, 'total_tokens': 228, 'completion_time': 0.081015457, 'prompt_time': 0.003669427, 'queue_time': 0.21722425, 'total_time': 0.084684884}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_77a9c76dc9', 'finish_reason': 'stop', 'logprobs': None} id='run-66562901-54ab-4e5a-ac76-680559ff27b7-0'\n",
            "Bot Answer for 'What specific research interests or academic goals do you hope to achieve during your time as a master’s student?': content=\"As a Master's student, Soe Htet Naing's research interests and academic goals primarily revolve around balancing advanced topics in the field of Data Science and AI. However, due to the lack of specific details provided in the context, I can only infer that his goals may involve further specialization in certain aspects of data science and artificial intelligence, possibly focusing on data-driven solutions, algorithm development, or the integration of these disciplines for innovative applications. Nonetheless, without explicit information from the context, these are educated assumptions based on the given academic disciplines. \" response_metadata={'token_usage': {'completion_tokens': 117, 'prompt_tokens': 125, 'total_tokens': 242, 'completion_time': 0.08263431, 'prompt_time': 0.003943773, 'queue_time': 0.235223894, 'total_time': 0.086578083}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_1612a676d6', 'finish_reason': 'stop', 'logprobs': None} id='run-6efa47b5-c9ac-4c8c-a97b-db3a1bc00943-0'\n"
          ]
        }
      ],
      "source": [
        "# Get responses for each question\n",
        "answers = []\n",
        "for question in questions:\n",
        "    response = groq_response(question)\n",
        "    answers.append(response)\n",
        "    print(f\"Bot Answer for '{question}':\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kpv0zXoGnwCI",
        "outputId": "456b28be-a90c-41ab-f5cd-a040538256f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"Soe Htet Naing's native language is Burmese. \" response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 184, 'total_tokens': 201, 'completion_time': 0.012095228, 'prompt_time': 0.005605992, 'queue_time': 0.46402824, 'total_time': 0.01770122}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_1612a676d6', 'finish_reason': 'stop', 'logprobs': None} id='run-36628d54-aa3c-48e6-96d5-32e466aea3e7-0'\n"
          ]
        }
      ],
      "source": [
        "print(groq_response(\"What is Soe Htet Naing native language?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK6TGwdbplJM",
        "outputId": "2f6a3dbf-6afd-47a1-9080-c94377a57247"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"To calculate Soe Htet Naing's age, we need to subtract his birth year and month from the current year and month. Since the current year is 2025 and the month is March, we will use these values for our calculation.\\n\\nSoe Htet Naing was born on July 15th, 1997. March 2025 comes after his birth month, so we will continue with the calculation using the remaining months of the year.\\n\\nFirst, let's subtract his birth year (1997) from the current year (2025):\\n2025 - 1997 = 28\\n\\nNow, we subtract his birth month (July) from the current month (March):\\nMarch - July = -4\\n\\nHowever, months cannot be negative, so we will consider the remaining days in March after subtracting July. Since March typically has around 31 days, and July has around 31 days as well, they should have the same number of days left after subtracting each other. Therefore, the result is 0.\\n\\nNow, we add the years together (28) and the remaining days (0):\\n28 + 0 = 28\\n\\nSoe Htet Naing is 28 years old in March 2025. \" response_metadata={'token_usage': {'completion_tokens': 297, 'prompt_tokens': 172, 'total_tokens': 469, 'completion_time': 0.208577666, 'prompt_time': 0.005261211, 'queue_time': 0.260880347, 'total_time': 0.213838877}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_1612a676d6', 'finish_reason': 'stop', 'logprobs': None} id='run-93df2133-1f17-4359-97cc-933ece1a35e9-0'\n"
          ]
        }
      ],
      "source": [
        "print(groq_response(\"How old is he if current year and month is 2025 and March?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk7ea3uN93Te",
        "outputId": "35d656e4-a688-4236-c093-782274678141"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"Soe Htet Naing possesses a diverse set of skills and strengths. Let's break them down:\\n\\n1. Soft Skills:\\n   a. Communication: Ability to effectively express ideas and understand others.\\n   b. Analytical Thinking: Capacity to analyze situations, gather information, and make sound decisions.\\n   c. Time Management: Ability to prioritize tasks, manage time efficiently, and meet deadlines.\\n   d. Adaptability: Flexibility to adjust to new situations and handle change.\\n   e. Collaboration: Ability to work effectively with others and contribute to team success.\\n\\n2. Technical Skills:\\n   a. Data Analysis: Proficiency in analyzing data, extracting insights, and presenting findings.\\n   b. AI & Machine Learning: Understanding and application of artificial intelligence and machine learning techniques.\\n   c. Python: Knowledge of the Python programming language, commonly used in data analysis and AI development.\\n   d. Microsoft Office: Familiarity with Microsoft Office suite, including Excel, Word, and PowerPoint.\\n\\n3. Professional Experience:\\n   - 5 years of experience across IT (Information Technology), management, and other areas, demonstrating versatility and adaptability in different roles.\\n\\nBased on the provided context, Soe Htet Naing's strengths lie in his soft skills such as communication, analytical thinking, time management, adaptability, and collaboration. Additionally, he possesses technical skills in data analysis, AI & machine learning, Python, and Microsoft Office. His professional experience spans across IT, management, and other areas, showcasing his adaptability and versatility in various roles. \" response_metadata={'token_usage': {'completion_tokens': 366, 'prompt_tokens': 137, 'total_tokens': 503, 'completion_time': 0.257135611, 'prompt_time': 0.004288197, 'queue_time': 0.469225913, 'total_time': 0.261423808}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_1612a676d6', 'finish_reason': 'stop', 'logprobs': None} id='run-d3a5b1a6-82e5-4d01-9b64-ce5f5029caeb-0'\n"
          ]
        }
      ],
      "source": [
        "print(groq_response(\"what are his skills and strength?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "pJVwkNrlGjCw"
      },
      "outputs": [],
      "source": [
        "# Load Groq Model (Use `llama3-70b` or `mixtral-8x7b`)\n",
        "groq_model = ChatGroq(model_name=\"allam-2-7b\", temperature=0.7)\n",
        "\n",
        "# Question Generator (Rewrites User Queries)\n",
        "question_generator = LLMChain(\n",
        "    llm=groq_model,\n",
        "    prompt=CONDENSE_QUESTION_PROMPT,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Retrieval Chain (Fetches Relevant Documents)\n",
        "doc_chain = load_qa_chain(\n",
        "    llm=groq_model,\n",
        "    chain_type=\"stuff\",\n",
        "    prompt=PROMPT,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Conversational Memory\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    k=3,\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    output_key=\"answer\"\n",
        ")\n",
        "\n",
        "# Conversational Retrieval Chain\n",
        "groq_chain = ConversationalRetrievalChain(\n",
        "    retriever=retriever,  # FAISS Retriever\n",
        "    question_generator=question_generator,\n",
        "    combine_docs_chain=doc_chain,\n",
        "    return_source_documents=True,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kWuFanus8kW",
        "outputId": "5792f07c-d501-4ba9-9b61-c9103417826a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "and results-driven professional with 5 years of\n",
            "\n",
            "Location: Yangon, Myanmar Date of Birth: 15th\n",
            "\n",
            "15th July 1997 PROFESSIONAL SUMMARY Motivated and\n",
            "\n",
            "Email: soehtetnaing.97@gmail.com Location:\n",
            "\n",
            "Question: How old are you?\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'How old are you?': {'question': 'How old are you?', 'chat_history': [], 'answer': \"Based on the provided context, Soe Htet Naing was born on 15th July 1997. To calculate their age, we subtract their birth year from the current year. However, as an AI, I can't access current date directly, but considering the year 2022, we would subtract 1997 from 2022. The calculation would be: 2022 - 1997 = 25 years old. Therefore, Soe Htet Naing is currently 25 years old. \", 'source_documents': [Document(page_content='and results-driven professional with 5 years of', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='Location: Yangon, Myanmar Date of Birth: 15th', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='15th July 1997 PROFESSIONAL SUMMARY Motivated and', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='Email: soehtetnaing.97@gmail.com Location:', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: How old are you?\n",
            "Assistant: Based on the provided context, Soe Htet Naing was born on 15th July 1997. To calculate their age, we subtract their birth year from the current year. However, as an AI, I can't access current date directly, but considering the year 2022, we would subtract 1997 from 2022. The calculation would be: 2022 - 1997 = 25 years old. Therefore, Soe Htet Naing is currently 25 years old. \n",
            "Follow Up Input: What is your highest level of education?\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "SOE HTET NAING\n",
            "\n",
            "No(5), Basic Education High School, Bothahtaung,\n",
            "\n",
            "Email: soehtetnaing.97@gmail.com Location:\n",
            "\n",
            "a Master of Science in Data Science and AI at\n",
            "\n",
            "Question: The standalone question would be: \"What is the highest level of education achieved by Soe Htet Naing?\"\n",
            "\n",
            "To arrive at this answer, I considered the context and information provided in the chat history. The previous question was about Soe Htet Naing's age, which has been clarified as 25 years old in 2022. When asked about the standalone question, I maintained the focus on Soe Htet Naing but shifted the topic to their educational background. Hence, I rephrased the question to inquire about their highest level of education, rather than their age. \n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'What is your highest level of education?': {'question': 'What is your highest level of education?', 'chat_history': [HumanMessage(content='How old are you?'), AIMessage(content=\"Based on the provided context, Soe Htet Naing was born on 15th July 1997. To calculate their age, we subtract their birth year from the current year. However, as an AI, I can't access current date directly, but considering the year 2022, we would subtract 1997 from 2022. The calculation would be: 2022 - 1997 = 25 years old. Therefore, Soe Htet Naing is currently 25 years old. \")], 'answer': \"Soe Htet Naing has attained a Master of Science degree, specifically in the fields of Data Science and AI. The context provided does not specify the name of the institution where this degree was obtained, but it does mention that Soe Htet Naing is currently pursuing this program. \\n\\nTo arrive at this answer, I focused on the part of the context that discusses Soe Htet Naing's educational background. It indicates that they are currently enrolled in a Master of Science program, which is the highest level of education mentioned in the context. Thus, based on the provided information, the question about Soe Htet Naing's highest level of education leads to the conclusion that they are studying for a Master's degree in Data Science and AI. \", 'source_documents': [Document(page_content='SOE HTET NAING', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='No(5), Basic Education High School, Bothahtaung,', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='Email: soehtetnaing.97@gmail.com Location:', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='a Master of Science in Data Science and AI at', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: How old are you?\n",
            "Assistant: Based on the provided context, Soe Htet Naing was born on 15th July 1997. To calculate their age, we subtract their birth year from the current year. However, as an AI, I can't access current date directly, but considering the year 2022, we would subtract 1997 from 2022. The calculation would be: 2022 - 1997 = 25 years old. Therefore, Soe Htet Naing is currently 25 years old. \n",
            "Human: What is your highest level of education?\n",
            "Assistant: Soe Htet Naing has attained a Master of Science degree, specifically in the fields of Data Science and AI. The context provided does not specify the name of the institution where this degree was obtained, but it does mention that Soe Htet Naing is currently pursuing this program. \n",
            "\n",
            "To arrive at this answer, I focused on the part of the context that discusses Soe Htet Naing's educational background. It indicates that they are currently enrolled in a Master of Science program, which is the highest level of education mentioned in the context. Thus, based on the provided information, the question about Soe Htet Naing's highest level of education leads to the conclusion that they are studying for a Master's degree in Data Science and AI. \n",
            "Follow Up Input: What major or field of study did you pursue during your education?\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "a Master of Science in Data Science and AI at\n",
            "\n",
            "SOE HTET NAING\n",
            "\n",
            "EDUCATION Master of Science in Data Science and\n",
            "\n",
            "Email: soehtetnaing.97@gmail.com Location:\n",
            "\n",
            "Question: What is the field of study for Soe Htet Naing's current Master's degree? \n",
            "\n",
            "To arrive at this answer, the original question asked about Soe Htet Naing's highest level of education. From the provided context, we learn that Soe Htet Naing is currently pursuing a Master of Science degree in Data Science and AI. Therefore, when rephrasing the question to standalone form, we ask about the specific field of study within this Master's degree, which is Data Science and AI. \n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'What major or field of study did you pursue during your education?': {'question': 'What major or field of study did you pursue during your education?', 'chat_history': [HumanMessage(content='How old are you?'), AIMessage(content=\"Based on the provided context, Soe Htet Naing was born on 15th July 1997. To calculate their age, we subtract their birth year from the current year. However, as an AI, I can't access current date directly, but considering the year 2022, we would subtract 1997 from 2022. The calculation would be: 2022 - 1997 = 25 years old. Therefore, Soe Htet Naing is currently 25 years old. \"), HumanMessage(content='What is your highest level of education?'), AIMessage(content=\"Soe Htet Naing has attained a Master of Science degree, specifically in the fields of Data Science and AI. The context provided does not specify the name of the institution where this degree was obtained, but it does mention that Soe Htet Naing is currently pursuing this program. \\n\\nTo arrive at this answer, I focused on the part of the context that discusses Soe Htet Naing's educational background. It indicates that they are currently enrolled in a Master of Science program, which is the highest level of education mentioned in the context. Thus, based on the provided information, the question about Soe Htet Naing's highest level of education leads to the conclusion that they are studying for a Master's degree in Data Science and AI. \")], 'answer': \"Soe Htet Naing is currently studying in the field of Data Science and AI for his Master of Science degree. \\n\\nTo elaborate, the context provided mentions that Soe Htet Naing is enrolled in a program called the Master of Science in Data Science and AI. This program focuses on the study of various aspects of data science and artificial intelligence, which include techniques for collecting, processing, analyzing, and interpreting data, as well as developing intelligent systems and applications. Thus, it is clear that Soe Htet Naing's area of study is within the realm of Data Science and AI. \", 'source_documents': [Document(page_content='a Master of Science in Data Science and AI at', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='SOE HTET NAING', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='EDUCATION Master of Science in Data Science and', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='Email: soehtetnaing.97@gmail.com Location:', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: How old are you?\n",
            "Assistant: Based on the provided context, Soe Htet Naing was born on 15th July 1997. To calculate their age, we subtract their birth year from the current year. However, as an AI, I can't access current date directly, but considering the year 2022, we would subtract 1997 from 2022. The calculation would be: 2022 - 1997 = 25 years old. Therefore, Soe Htet Naing is currently 25 years old. \n",
            "Human: What is your highest level of education?\n",
            "Assistant: Soe Htet Naing has attained a Master of Science degree, specifically in the fields of Data Science and AI. The context provided does not specify the name of the institution where this degree was obtained, but it does mention that Soe Htet Naing is currently pursuing this program. \n",
            "\n",
            "To arrive at this answer, I focused on the part of the context that discusses Soe Htet Naing's educational background. It indicates that they are currently enrolled in a Master of Science program, which is the highest level of education mentioned in the context. Thus, based on the provided information, the question about Soe Htet Naing's highest level of education leads to the conclusion that they are studying for a Master's degree in Data Science and AI. \n",
            "Human: What major or field of study did you pursue during your education?\n",
            "Assistant: Soe Htet Naing is currently studying in the field of Data Science and AI for his Master of Science degree. \n",
            "\n",
            "To elaborate, the context provided mentions that Soe Htet Naing is enrolled in a program called the Master of Science in Data Science and AI. This program focuses on the study of various aspects of data science and artificial intelligence, which include techniques for collecting, processing, analyzing, and interpreting data, as well as developing intelligent systems and applications. Thus, it is clear that Soe Htet Naing's area of study is within the realm of Data Science and AI. \n",
            "Follow Up Input: How many years of work experience do you have?\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "years of experience across IT, management, and\n",
            "\n",
            "SOE HTET NAING\n",
            "\n",
            "and results-driven professional with 5 years of\n",
            "\n",
            "Yangon, Myanmar WORK EXPERIENCE General Manager\n",
            "\n",
            "Question: The original question \"How many years of work experience do you have?\" is seeking information about Soe Htet Naing's work experience. To rephrase this as a standalone question, we would focus on asking directly about their work experience without mentioning any specific follow-up information. \n",
            "\n",
            "Therefore, the standalone question would be: \"What is your work experience?\" \n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'How many years of work experience do you have?': {'question': 'How many years of work experience do you have?', 'chat_history': [HumanMessage(content='How old are you?'), AIMessage(content=\"Based on the provided context, Soe Htet Naing was born on 15th July 1997. To calculate their age, we subtract their birth year from the current year. However, as an AI, I can't access current date directly, but considering the year 2022, we would subtract 1997 from 2022. The calculation would be: 2022 - 1997 = 25 years old. Therefore, Soe Htet Naing is currently 25 years old. \"), HumanMessage(content='What is your highest level of education?'), AIMessage(content=\"Soe Htet Naing has attained a Master of Science degree, specifically in the fields of Data Science and AI. The context provided does not specify the name of the institution where this degree was obtained, but it does mention that Soe Htet Naing is currently pursuing this program. \\n\\nTo arrive at this answer, I focused on the part of the context that discusses Soe Htet Naing's educational background. It indicates that they are currently enrolled in a Master of Science program, which is the highest level of education mentioned in the context. Thus, based on the provided information, the question about Soe Htet Naing's highest level of education leads to the conclusion that they are studying for a Master's degree in Data Science and AI. \"), HumanMessage(content='What major or field of study did you pursue during your education?'), AIMessage(content=\"Soe Htet Naing is currently studying in the field of Data Science and AI for his Master of Science degree. \\n\\nTo elaborate, the context provided mentions that Soe Htet Naing is enrolled in a program called the Master of Science in Data Science and AI. This program focuses on the study of various aspects of data science and artificial intelligence, which include techniques for collecting, processing, analyzing, and interpreting data, as well as developing intelligent systems and applications. Thus, it is clear that Soe Htet Naing's area of study is within the realm of Data Science and AI. \")], 'answer': 'Soe Htet Naing possesses approximately 5 years of work experience, as mentioned in the provided context. The context highlights that he is a results-driven professional with a background spanning various domains such as IT, management, and more. His impressive tenure includes serving as the General Manager in Yangon, Myanmar. Specifically, focusing on his professional journey, we can deduce that Soe Htet Naing has amassed roughly 5 years of work experience. ', 'source_documents': [Document(page_content='years of experience across IT, management, and', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='SOE HTET NAING', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='and results-driven professional with 5 years of', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='Yangon, Myanmar WORK EXPERIENCE General Manager', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What is your highest level of education?\n",
            "Assistant: Soe Htet Naing has attained a Master of Science degree, specifically in the fields of Data Science and AI. The context provided does not specify the name of the institution where this degree was obtained, but it does mention that Soe Htet Naing is currently pursuing this program. \n",
            "\n",
            "To arrive at this answer, I focused on the part of the context that discusses Soe Htet Naing's educational background. It indicates that they are currently enrolled in a Master of Science program, which is the highest level of education mentioned in the context. Thus, based on the provided information, the question about Soe Htet Naing's highest level of education leads to the conclusion that they are studying for a Master's degree in Data Science and AI. \n",
            "Human: What major or field of study did you pursue during your education?\n",
            "Assistant: Soe Htet Naing is currently studying in the field of Data Science and AI for his Master of Science degree. \n",
            "\n",
            "To elaborate, the context provided mentions that Soe Htet Naing is enrolled in a program called the Master of Science in Data Science and AI. This program focuses on the study of various aspects of data science and artificial intelligence, which include techniques for collecting, processing, analyzing, and interpreting data, as well as developing intelligent systems and applications. Thus, it is clear that Soe Htet Naing's area of study is within the realm of Data Science and AI. \n",
            "Human: How many years of work experience do you have?\n",
            "Assistant: Soe Htet Naing possesses approximately 5 years of work experience, as mentioned in the provided context. The context highlights that he is a results-driven professional with a background spanning various domains such as IT, management, and more. His impressive tenure includes serving as the General Manager in Yangon, Myanmar. Specifically, focusing on his professional journey, we can deduce that Soe Htet Naing has amassed roughly 5 years of work experience. \n",
            "Follow Up Input: What type of work or industry have you been involved in?\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "SOE HTET NAING\n",
            "\n",
            "years of experience across IT, management, and\n",
            "\n",
            "and entertainment industries. Adept at business\n",
            "\n",
            "industry applications. - Staying updated with\n",
            "\n",
            "Question: What specific industries or work domains has Soe Htet Naing worked in or been involved with? \n",
            "\n",
            "To arrive at this question, I analyzed the original follow-up input which asked about Soe Htet Naing's work or industry experience. I then rephrased the question to focus on the specific industries or work domains Soe Htet Naing has engaged with, rather than asking about his overall work experience or industry involvement. This change ensures the question remains standalone and does not rely on the previous answer about years of work experience. \n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'What type of work or industry have you been involved in?': {'question': 'What type of work or industry have you been involved in?', 'chat_history': [HumanMessage(content='What is your highest level of education?'), AIMessage(content=\"Soe Htet Naing has attained a Master of Science degree, specifically in the fields of Data Science and AI. The context provided does not specify the name of the institution where this degree was obtained, but it does mention that Soe Htet Naing is currently pursuing this program. \\n\\nTo arrive at this answer, I focused on the part of the context that discusses Soe Htet Naing's educational background. It indicates that they are currently enrolled in a Master of Science program, which is the highest level of education mentioned in the context. Thus, based on the provided information, the question about Soe Htet Naing's highest level of education leads to the conclusion that they are studying for a Master's degree in Data Science and AI. \"), HumanMessage(content='What major or field of study did you pursue during your education?'), AIMessage(content=\"Soe Htet Naing is currently studying in the field of Data Science and AI for his Master of Science degree. \\n\\nTo elaborate, the context provided mentions that Soe Htet Naing is enrolled in a program called the Master of Science in Data Science and AI. This program focuses on the study of various aspects of data science and artificial intelligence, which include techniques for collecting, processing, analyzing, and interpreting data, as well as developing intelligent systems and applications. Thus, it is clear that Soe Htet Naing's area of study is within the realm of Data Science and AI. \"), HumanMessage(content='How many years of work experience do you have?'), AIMessage(content='Soe Htet Naing possesses approximately 5 years of work experience, as mentioned in the provided context. The context highlights that he is a results-driven professional with a background spanning various domains such as IT, management, and more. His impressive tenure includes serving as the General Manager in Yangon, Myanmar. Specifically, focusing on his professional journey, we can deduce that Soe Htet Naing has amassed roughly 5 years of work experience. ')], 'answer': 'Soe Htet Naing has worked in or been involved with the following industries or work domains: IT (Information Technology), management, and entertainment. He possesses expertise in business industry applications and remains updated with the latest developments in these areas. ', 'source_documents': [Document(page_content='SOE HTET NAING', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='years of experience across IT, management, and', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='and entertainment industries. Adept at business', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='industry applications. - Staying updated with', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What major or field of study did you pursue during your education?\n",
            "Assistant: Soe Htet Naing is currently studying in the field of Data Science and AI for his Master of Science degree. \n",
            "\n",
            "To elaborate, the context provided mentions that Soe Htet Naing is enrolled in a program called the Master of Science in Data Science and AI. This program focuses on the study of various aspects of data science and artificial intelligence, which include techniques for collecting, processing, analyzing, and interpreting data, as well as developing intelligent systems and applications. Thus, it is clear that Soe Htet Naing's area of study is within the realm of Data Science and AI. \n",
            "Human: How many years of work experience do you have?\n",
            "Assistant: Soe Htet Naing possesses approximately 5 years of work experience, as mentioned in the provided context. The context highlights that he is a results-driven professional with a background spanning various domains such as IT, management, and more. His impressive tenure includes serving as the General Manager in Yangon, Myanmar. Specifically, focusing on his professional journey, we can deduce that Soe Htet Naing has amassed roughly 5 years of work experience. \n",
            "Human: What type of work or industry have you been involved in?\n",
            "Assistant: Soe Htet Naing has worked in or been involved with the following industries or work domains: IT (Information Technology), management, and entertainment. He possesses expertise in business industry applications and remains updated with the latest developments in these areas. \n",
            "Follow Up Input: Can you describe your current role or job responsibilities?\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "SOE HTET NAING\n",
            "\n",
            "Email: soehtetnaing.97@gmail.com Location:\n",
            "\n",
            "subtitles and performed quality assurance. -\n",
            "\n",
            "15th July 1997 PROFESSIONAL SUMMARY Motivated and\n",
            "\n",
            "Question: The standalone question would be: \"What are Soe Htet Naing's job responsibilities or role currently?\" \n",
            "\n",
            "To arrive at this question, I considered the context provided in the chat history. The conversation focuses on Soe Htet Naing's educational background, work experience, and industry involvement. Given that his current role or job responsibilities are mentioned in the conversation, it makes sense to create a standalone question that directly asks about these aspects. This ensures that the question remains relevant to the provided context and doesn't require additional information or clarification from the conversation history. \n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'Can you describe your current role or job responsibilities?': {'question': 'Can you describe your current role or job responsibilities?', 'chat_history': [HumanMessage(content='What major or field of study did you pursue during your education?'), AIMessage(content=\"Soe Htet Naing is currently studying in the field of Data Science and AI for his Master of Science degree. \\n\\nTo elaborate, the context provided mentions that Soe Htet Naing is enrolled in a program called the Master of Science in Data Science and AI. This program focuses on the study of various aspects of data science and artificial intelligence, which include techniques for collecting, processing, analyzing, and interpreting data, as well as developing intelligent systems and applications. Thus, it is clear that Soe Htet Naing's area of study is within the realm of Data Science and AI. \"), HumanMessage(content='How many years of work experience do you have?'), AIMessage(content='Soe Htet Naing possesses approximately 5 years of work experience, as mentioned in the provided context. The context highlights that he is a results-driven professional with a background spanning various domains such as IT, management, and more. His impressive tenure includes serving as the General Manager in Yangon, Myanmar. Specifically, focusing on his professional journey, we can deduce that Soe Htet Naing has amassed roughly 5 years of work experience. '), HumanMessage(content='What type of work or industry have you been involved in?'), AIMessage(content='Soe Htet Naing has worked in or been involved with the following industries or work domains: IT (Information Technology), management, and entertainment. He possesses expertise in business industry applications and remains updated with the latest developments in these areas. ')], 'answer': \"Based on the information provided in the context, Soe Htet Naing's job responsibilities or role currently involve working with subtitles and performing quality assurance. These tasks likely fall within the domain of video or media production, where subtitling and quality control are essential processes. However, without further details, we cannot pinpoint the exact nature of his job or organization he is associated with. \", 'source_documents': [Document(page_content='SOE HTET NAING', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='Email: soehtetnaing.97@gmail.com Location:', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='subtitles and performed quality assurance. -', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='15th July 1997 PROFESSIONAL SUMMARY Motivated and', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: How many years of work experience do you have?\n",
            "Assistant: Soe Htet Naing possesses approximately 5 years of work experience, as mentioned in the provided context. The context highlights that he is a results-driven professional with a background spanning various domains such as IT, management, and more. His impressive tenure includes serving as the General Manager in Yangon, Myanmar. Specifically, focusing on his professional journey, we can deduce that Soe Htet Naing has amassed roughly 5 years of work experience. \n",
            "Human: What type of work or industry have you been involved in?\n",
            "Assistant: Soe Htet Naing has worked in or been involved with the following industries or work domains: IT (Information Technology), management, and entertainment. He possesses expertise in business industry applications and remains updated with the latest developments in these areas. \n",
            "Human: Can you describe your current role or job responsibilities?\n",
            "Assistant: Based on the information provided in the context, Soe Htet Naing's job responsibilities or role currently involve working with subtitles and performing quality assurance. These tasks likely fall within the domain of video or media production, where subtitling and quality control are essential processes. However, without further details, we cannot pinpoint the exact nature of his job or organization he is associated with. \n",
            "Follow Up Input: What are your core beliefs regarding the role of technology in shaping society?\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "in Society: Technology plays a critical role in\n",
            "\n",
            "PERSONAL PERSPECTIVES Role of Technology in\n",
            "\n",
            "technology to shape the future while integrating\n",
            "\n",
            "SOE HTET NAING\n",
            "\n",
            "Question: The original question seems to be a part of a conversation where the assistant is providing information about Soe Htet Naing. The question \"What are your core beliefs regarding the role of technology in shaping society?\" appears to be a continuation of this conversation, seeking insights about Soe Htet Naing's personal views.\n",
            "\n",
            "To rephrase this question as a standalone question, we should remove any contextual cues related to the previous conversation. The standalone question would be:\n",
            "\n",
            "\"What are your core beliefs regarding the role of technology in shaping society?\" \n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'What are your core beliefs regarding the role of technology in shaping society?': {'question': 'What are your core beliefs regarding the role of technology in shaping society?', 'chat_history': [HumanMessage(content='How many years of work experience do you have?'), AIMessage(content='Soe Htet Naing possesses approximately 5 years of work experience, as mentioned in the provided context. The context highlights that he is a results-driven professional with a background spanning various domains such as IT, management, and more. His impressive tenure includes serving as the General Manager in Yangon, Myanmar. Specifically, focusing on his professional journey, we can deduce that Soe Htet Naing has amassed roughly 5 years of work experience. '), HumanMessage(content='What type of work or industry have you been involved in?'), AIMessage(content='Soe Htet Naing has worked in or been involved with the following industries or work domains: IT (Information Technology), management, and entertainment. He possesses expertise in business industry applications and remains updated with the latest developments in these areas. '), HumanMessage(content='Can you describe your current role or job responsibilities?'), AIMessage(content=\"Based on the information provided in the context, Soe Htet Naing's job responsibilities or role currently involve working with subtitles and performing quality assurance. These tasks likely fall within the domain of video or media production, where subtitling and quality control are essential processes. However, without further details, we cannot pinpoint the exact nature of his job or organization he is associated with. \")], 'answer': \"I don't have direct insights into Soe Htet Naing's personal beliefs. However, I can provide some general information about the topic of technology's impact on society. The role of technology in shaping society is a widely debated issue. Some people believe that technology has brought about significant improvements in various aspects of life, such as communication, education, and efficiency. They argue that technology has empowered individuals and enabled us to access vast amounts of information and connect with people across the globe.\\n\\nOn the other hand, others express concerns about the downsides of technology, such as job displacement due to automation, privacy invasion, and the potential for misuse of data. These concerns often stem from worries about the negative impacts on humanity, society, and the environment.\\n\\nAs for Soe Htet Naing's beliefs on the subject, we would require specific information about their thoughts or opinions from reliable sources. \", 'source_documents': [Document(page_content='in Society: Technology plays a critical role in', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='PERSONAL PERSPECTIVES Role of Technology in', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='technology to shape the future while integrating', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='SOE HTET NAING', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What type of work or industry have you been involved in?\n",
            "Assistant: Soe Htet Naing has worked in or been involved with the following industries or work domains: IT (Information Technology), management, and entertainment. He possesses expertise in business industry applications and remains updated with the latest developments in these areas. \n",
            "Human: Can you describe your current role or job responsibilities?\n",
            "Assistant: Based on the information provided in the context, Soe Htet Naing's job responsibilities or role currently involve working with subtitles and performing quality assurance. These tasks likely fall within the domain of video or media production, where subtitling and quality control are essential processes. However, without further details, we cannot pinpoint the exact nature of his job or organization he is associated with. \n",
            "Human: What are your core beliefs regarding the role of technology in shaping society?\n",
            "Assistant: I don't have direct insights into Soe Htet Naing's personal beliefs. However, I can provide some general information about the topic of technology's impact on society. The role of technology in shaping society is a widely debated issue. Some people believe that technology has brought about significant improvements in various aspects of life, such as communication, education, and efficiency. They argue that technology has empowered individuals and enabled us to access vast amounts of information and connect with people across the globe.\n",
            "\n",
            "On the other hand, others express concerns about the downsides of technology, such as job displacement due to automation, privacy invasion, and the potential for misuse of data. These concerns often stem from worries about the negative impacts on humanity, society, and the environment.\n",
            "\n",
            "As for Soe Htet Naing's beliefs on the subject, we would require specific information about their thoughts or opinions from reliable sources. \n",
            "Follow Up Input: How do you think cultural values should influence technological advancements?\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "Advancements: Cultural values should serve as a\n",
            "\n",
            "cultural values to ensure ethical advancements.\n",
            "\n",
            "AI deployment. Cultural Values & Technological\n",
            "\n",
            "PERSONAL PERSPECTIVES Role of Technology in\n",
            "\n",
            "Question: Can you provide your personal viewpoint on how cultural values should impact technological advancements? \n",
            "\n",
            "To rephrase the standalone question, I considered the original question \"How do you think cultural values should influence technological advancements?\" which seems to be seeking the user's perspective on this matter. However, the standalone question should inquire about my perspective, not a rephrased version of the user's question. Therefore, I rephrased the question as \"Please share your opinion on how cultural values should affect technological progress.\" \n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'How do you think cultural values should influence technological advancements?': {'question': 'How do you think cultural values should influence technological advancements?', 'chat_history': [HumanMessage(content='What type of work or industry have you been involved in?'), AIMessage(content='Soe Htet Naing has worked in or been involved with the following industries or work domains: IT (Information Technology), management, and entertainment. He possesses expertise in business industry applications and remains updated with the latest developments in these areas. '), HumanMessage(content='Can you describe your current role or job responsibilities?'), AIMessage(content=\"Based on the information provided in the context, Soe Htet Naing's job responsibilities or role currently involve working with subtitles and performing quality assurance. These tasks likely fall within the domain of video or media production, where subtitling and quality control are essential processes. However, without further details, we cannot pinpoint the exact nature of his job or organization he is associated with. \"), HumanMessage(content='What are your core beliefs regarding the role of technology in shaping society?'), AIMessage(content=\"I don't have direct insights into Soe Htet Naing's personal beliefs. However, I can provide some general information about the topic of technology's impact on society. The role of technology in shaping society is a widely debated issue. Some people believe that technology has brought about significant improvements in various aspects of life, such as communication, education, and efficiency. They argue that technology has empowered individuals and enabled us to access vast amounts of information and connect with people across the globe.\\n\\nOn the other hand, others express concerns about the downsides of technology, such as job displacement due to automation, privacy invasion, and the potential for misuse of data. These concerns often stem from worries about the negative impacts on humanity, society, and the environment.\\n\\nAs for Soe Htet Naing's beliefs on the subject, we would require specific information about their thoughts or opinions from reliable sources. \")], 'answer': 'In my viewpoint, I firmly believe that cultural values should play a crucial role in shaping technological advancements. Cultural values serve as a fundamental foundation that shapes our societal norms, ethics, and principles. By incorporating these values into the development of new technologies, we ensure that these advancements contribute positively to our society.\\n\\nTechnology, when designed within the framework of cultural values, can help maintain harmony among individuals, promote equality, protect human rights, and preserve our environment. For instance, technologies that prioritize transparency, accountability, and respect for diverse perspectives can help build trust among people and foster inclusive growth.\\n\\nMoreover, incorporating cultural values in AI development is particularly vital. Artificial intelligence systems should be designed to embody respect for human dignity, preservation of cultural heritage, and protection of individual privacy. This approach ensures that AI technologies do not undermine the integrity of our cultures and societies.\\n\\nIn conclusion, by integrating cultural values into technological advancements, we can ensure that these innovations not only improve our lives but also align with our shared societal values and aspirations. ', 'source_documents': [Document(page_content='Advancements: Cultural values should serve as a', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='cultural values to ensure ethical advancements.', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0}), Document(page_content='AI deployment. Cultural Values & Technological', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='PERSONAL PERSPECTIVES Role of Technology in', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Can you describe your current role or job responsibilities?\n",
            "Assistant: Based on the information provided in the context, Soe Htet Naing's job responsibilities or role currently involve working with subtitles and performing quality assurance. These tasks likely fall within the domain of video or media production, where subtitling and quality control are essential processes. However, without further details, we cannot pinpoint the exact nature of his job or organization he is associated with. \n",
            "Human: What are your core beliefs regarding the role of technology in shaping society?\n",
            "Assistant: I don't have direct insights into Soe Htet Naing's personal beliefs. However, I can provide some general information about the topic of technology's impact on society. The role of technology in shaping society is a widely debated issue. Some people believe that technology has brought about significant improvements in various aspects of life, such as communication, education, and efficiency. They argue that technology has empowered individuals and enabled us to access vast amounts of information and connect with people across the globe.\n",
            "\n",
            "On the other hand, others express concerns about the downsides of technology, such as job displacement due to automation, privacy invasion, and the potential for misuse of data. These concerns often stem from worries about the negative impacts on humanity, society, and the environment.\n",
            "\n",
            "As for Soe Htet Naing's beliefs on the subject, we would require specific information about their thoughts or opinions from reliable sources. \n",
            "Human: How do you think cultural values should influence technological advancements?\n",
            "Assistant: In my viewpoint, I firmly believe that cultural values should play a crucial role in shaping technological advancements. Cultural values serve as a fundamental foundation that shapes our societal norms, ethics, and principles. By incorporating these values into the development of new technologies, we ensure that these advancements contribute positively to our society.\n",
            "\n",
            "Technology, when designed within the framework of cultural values, can help maintain harmony among individuals, promote equality, protect human rights, and preserve our environment. For instance, technologies that prioritize transparency, accountability, and respect for diverse perspectives can help build trust among people and foster inclusive growth.\n",
            "\n",
            "Moreover, incorporating cultural values in AI development is particularly vital. Artificial intelligence systems should be designed to embody respect for human dignity, preservation of cultural heritage, and protection of individual privacy. This approach ensures that AI technologies do not undermine the integrity of our cultures and societies.\n",
            "\n",
            "In conclusion, by integrating cultural values into technological advancements, we can ensure that these innovations not only improve our lives but also align with our shared societal values and aspirations. \n",
            "Follow Up Input: As a master’s student, what is the most challenging aspect of your studies so far?\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "AI deployment. Cultural Values & Technological\n",
            "\n",
            "as a guiding framework for AI and technological\n",
            "\n",
            "societal impact. - Exploring explainable AI (XAI)\n",
            "\n",
            "Advancements: Cultural values should serve as a\n",
            "\n",
            "Question: A standalone question based on the given conversation could be: \"Can you provide an overview of the discussed topics related to technology, society, and cultural values?\" \n",
            "\n",
            "This question is derived from the latter part of the conversation where the AI assistant discusses the importance of cultural values in shaping technology and the need for AI systems to respect human dignity, cultural heritage, and privacy. The question is designed to elicit a general discussion about technology's impact on society and culture, which aligns with the broader theme of the conversation. \n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'As a master’s student, what is the most challenging aspect of your studies so far?': {'question': 'As a master’s student, what is the most challenging aspect of your studies so far?', 'chat_history': [HumanMessage(content='Can you describe your current role or job responsibilities?'), AIMessage(content=\"Based on the information provided in the context, Soe Htet Naing's job responsibilities or role currently involve working with subtitles and performing quality assurance. These tasks likely fall within the domain of video or media production, where subtitling and quality control are essential processes. However, without further details, we cannot pinpoint the exact nature of his job or organization he is associated with. \"), HumanMessage(content='What are your core beliefs regarding the role of technology in shaping society?'), AIMessage(content=\"I don't have direct insights into Soe Htet Naing's personal beliefs. However, I can provide some general information about the topic of technology's impact on society. The role of technology in shaping society is a widely debated issue. Some people believe that technology has brought about significant improvements in various aspects of life, such as communication, education, and efficiency. They argue that technology has empowered individuals and enabled us to access vast amounts of information and connect with people across the globe.\\n\\nOn the other hand, others express concerns about the downsides of technology, such as job displacement due to automation, privacy invasion, and the potential for misuse of data. These concerns often stem from worries about the negative impacts on humanity, society, and the environment.\\n\\nAs for Soe Htet Naing's beliefs on the subject, we would require specific information about their thoughts or opinions from reliable sources. \"), HumanMessage(content='How do you think cultural values should influence technological advancements?'), AIMessage(content='In my viewpoint, I firmly believe that cultural values should play a crucial role in shaping technological advancements. Cultural values serve as a fundamental foundation that shapes our societal norms, ethics, and principles. By incorporating these values into the development of new technologies, we ensure that these advancements contribute positively to our society.\\n\\nTechnology, when designed within the framework of cultural values, can help maintain harmony among individuals, promote equality, protect human rights, and preserve our environment. For instance, technologies that prioritize transparency, accountability, and respect for diverse perspectives can help build trust among people and foster inclusive growth.\\n\\nMoreover, incorporating cultural values in AI development is particularly vital. Artificial intelligence systems should be designed to embody respect for human dignity, preservation of cultural heritage, and protection of individual privacy. This approach ensures that AI technologies do not undermine the integrity of our cultures and societies.\\n\\nIn conclusion, by integrating cultural values into technological advancements, we can ensure that these innovations not only improve our lives but also align with our shared societal values and aspirations. ')], 'answer': \"Certainly, I'll provide a comprehensive overview of the discussed topics regarding technology, society, and cultural values. \\n\\nThe conversation revolves around the concept of AI deployment and the significance of cultural values as a guiding framework for technological advancements, particularly in terms of societal impact. One prominent topic is the exploration of explainable AI (XAI), which seeks to develop AI systems that can provide transparent and comprehensible explanations for their decisions.\\n\\nAnother important aspect discussed is the integration of cultural values into AI systems. This involves ensuring that AI respects human dignity, acknowledges and preserves cultural heritage, and protects users' privacy. By incorporating cultural values into AI systems, we can ensure that technology aligns with human values and does not inadvertently harm or distort cultural traditions.\\n\\nIn essence, the conversation highlights the need for responsible AI development that considers not only technological advancements but also the cultural and societal implications of these advancements. It emphasizes the importance of understanding and respecting diverse cultural values and how this understanding can contribute to the design of ethical AI systems. \", 'source_documents': [Document(page_content='AI deployment. Cultural Values & Technological', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='as a guiding framework for AI and technological', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='societal impact. - Exploring explainable AI (XAI)', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='Advancements: Cultural values should serve as a', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What are your core beliefs regarding the role of technology in shaping society?\n",
            "Assistant: I don't have direct insights into Soe Htet Naing's personal beliefs. However, I can provide some general information about the topic of technology's impact on society. The role of technology in shaping society is a widely debated issue. Some people believe that technology has brought about significant improvements in various aspects of life, such as communication, education, and efficiency. They argue that technology has empowered individuals and enabled us to access vast amounts of information and connect with people across the globe.\n",
            "\n",
            "On the other hand, others express concerns about the downsides of technology, such as job displacement due to automation, privacy invasion, and the potential for misuse of data. These concerns often stem from worries about the negative impacts on humanity, society, and the environment.\n",
            "\n",
            "As for Soe Htet Naing's beliefs on the subject, we would require specific information about their thoughts or opinions from reliable sources. \n",
            "Human: How do you think cultural values should influence technological advancements?\n",
            "Assistant: In my viewpoint, I firmly believe that cultural values should play a crucial role in shaping technological advancements. Cultural values serve as a fundamental foundation that shapes our societal norms, ethics, and principles. By incorporating these values into the development of new technologies, we ensure that these advancements contribute positively to our society.\n",
            "\n",
            "Technology, when designed within the framework of cultural values, can help maintain harmony among individuals, promote equality, protect human rights, and preserve our environment. For instance, technologies that prioritize transparency, accountability, and respect for diverse perspectives can help build trust among people and foster inclusive growth.\n",
            "\n",
            "Moreover, incorporating cultural values in AI development is particularly vital. Artificial intelligence systems should be designed to embody respect for human dignity, preservation of cultural heritage, and protection of individual privacy. This approach ensures that AI technologies do not undermine the integrity of our cultures and societies.\n",
            "\n",
            "In conclusion, by integrating cultural values into technological advancements, we can ensure that these innovations not only improve our lives but also align with our shared societal values and aspirations. \n",
            "Human: As a master’s student, what is the most challenging aspect of your studies so far?\n",
            "Assistant: Certainly, I'll provide a comprehensive overview of the discussed topics regarding technology, society, and cultural values. \n",
            "\n",
            "The conversation revolves around the concept of AI deployment and the significance of cultural values as a guiding framework for technological advancements, particularly in terms of societal impact. One prominent topic is the exploration of explainable AI (XAI), which seeks to develop AI systems that can provide transparent and comprehensible explanations for their decisions.\n",
            "\n",
            "Another important aspect discussed is the integration of cultural values into AI systems. This involves ensuring that AI respects human dignity, acknowledges and preserves cultural heritage, and protects users' privacy. By incorporating cultural values into AI systems, we can ensure that technology aligns with human values and does not inadvertently harm or distort cultural traditions.\n",
            "\n",
            "In essence, the conversation highlights the need for responsible AI development that considers not only technological advancements but also the cultural and societal implications of these advancements. It emphasizes the importance of understanding and respecting diverse cultural values and how this understanding can contribute to the design of ethical AI systems. \n",
            "Follow Up Input: What specific research interests or academic goals do you hope to achieve during your time as a master’s student?\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease answer the following question accurately based on the provided context of a person named Soe Htet Naing.\n",
            "Context:\n",
            "AI deployment. Cultural Values & Technological\n",
            "\n",
            "societal impact. - Exploring explainable AI (XAI)\n",
            "\n",
            "AI (XAI) and ethical AI practices in real-world\n",
            "\n",
            "cultural values to ensure ethical advancements.\n",
            "\n",
            "Question: What are the main topics discussed in the conversation, such as explainable AI and integrating cultural values into AI systems? \n",
            "\n",
            "The conversation revolves around two primary subjects: the development of AI systems that can provide clear justifications for their decisions (explainable AI, or XAI), and integrating cultural values into these systems to ensure they respect human dignity, protect cultural heritage, and maintain user privacy. The discussion emphasizes the importance of considering both technological advancements and the societal and cultural implications of these advancements when developing ethical AI systems. \n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'What specific research interests or academic goals do you hope to achieve during your time as a master’s student?': {'question': 'What specific research interests or academic goals do you hope to achieve during your time as a master’s student?', 'chat_history': [HumanMessage(content='What are your core beliefs regarding the role of technology in shaping society?'), AIMessage(content=\"I don't have direct insights into Soe Htet Naing's personal beliefs. However, I can provide some general information about the topic of technology's impact on society. The role of technology in shaping society is a widely debated issue. Some people believe that technology has brought about significant improvements in various aspects of life, such as communication, education, and efficiency. They argue that technology has empowered individuals and enabled us to access vast amounts of information and connect with people across the globe.\\n\\nOn the other hand, others express concerns about the downsides of technology, such as job displacement due to automation, privacy invasion, and the potential for misuse of data. These concerns often stem from worries about the negative impacts on humanity, society, and the environment.\\n\\nAs for Soe Htet Naing's beliefs on the subject, we would require specific information about their thoughts or opinions from reliable sources. \"), HumanMessage(content='How do you think cultural values should influence technological advancements?'), AIMessage(content='In my viewpoint, I firmly believe that cultural values should play a crucial role in shaping technological advancements. Cultural values serve as a fundamental foundation that shapes our societal norms, ethics, and principles. By incorporating these values into the development of new technologies, we ensure that these advancements contribute positively to our society.\\n\\nTechnology, when designed within the framework of cultural values, can help maintain harmony among individuals, promote equality, protect human rights, and preserve our environment. For instance, technologies that prioritize transparency, accountability, and respect for diverse perspectives can help build trust among people and foster inclusive growth.\\n\\nMoreover, incorporating cultural values in AI development is particularly vital. Artificial intelligence systems should be designed to embody respect for human dignity, preservation of cultural heritage, and protection of individual privacy. This approach ensures that AI technologies do not undermine the integrity of our cultures and societies.\\n\\nIn conclusion, by integrating cultural values into technological advancements, we can ensure that these innovations not only improve our lives but also align with our shared societal values and aspirations. '), HumanMessage(content='As a master’s student, what is the most challenging aspect of your studies so far?'), AIMessage(content=\"Certainly, I'll provide a comprehensive overview of the discussed topics regarding technology, society, and cultural values. \\n\\nThe conversation revolves around the concept of AI deployment and the significance of cultural values as a guiding framework for technological advancements, particularly in terms of societal impact. One prominent topic is the exploration of explainable AI (XAI), which seeks to develop AI systems that can provide transparent and comprehensible explanations for their decisions.\\n\\nAnother important aspect discussed is the integration of cultural values into AI systems. This involves ensuring that AI respects human dignity, acknowledges and preserves cultural heritage, and protects users' privacy. By incorporating cultural values into AI systems, we can ensure that technology aligns with human values and does not inadvertently harm or distort cultural traditions.\\n\\nIn essence, the conversation highlights the need for responsible AI development that considers not only technological advancements but also the cultural and societal implications of these advancements. It emphasizes the importance of understanding and respecting diverse cultural values and how this understanding can contribute to the design of ethical AI systems. \")], 'answer': 'The key topics in this discussion, as per the provided context, are:\\n\\n1. Explainable AI (XAI): This refers to the aspect of AI systems that allows humans to understand the reasoning behind the decisions made by these systems. The discussion likely revolves around the development and implications of XAI technology.\\n\\n2. Cultural Values in AI Systems: The conversation highlights the significance of incorporating cultural values into AI development. This involves ensuring that AI respects human dignity, safeguards cultural heritage, and protects user privacy, thereby maintaining the proper functioning of AI within diverse cultural contexts.\\n\\nThese two topics are intertwined, as the integration of cultural values into AI systems can contribute to the development of more ethical and understandable AI, which falls under the broader theme of XAI. \\n\\nIn essence, the discussion covers aspects of AI development that aim to strike a balance between technological advancements and the societal, ethical, and cultural implications of AI systems. ', 'source_documents': [Document(page_content='AI deployment. Cultural Values & Technological', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='societal impact. - Exploring explainable AI (XAI)', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='AI (XAI) and ethical AI practices in real-world', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 1}), Document(page_content='cultural values to ensure ethical advancements.', metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/Soe_Htet_Naing_Resume.pdf', 'page': 0})]}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Get responses for each question\n",
        "answers = []\n",
        "for question in questions:\n",
        "    response = groq_chain.invoke({\"question\": question})\n",
        "    answers.append(response)\n",
        "    print(f\"Bot Answer for '{question}':\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2erStZxxc6B"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
